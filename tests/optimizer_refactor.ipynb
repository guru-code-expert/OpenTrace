{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e6d239b8d39ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T19:28:42.912695Z",
     "start_time": "2025-07-09T19:28:42.902831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40158633-4cb9-47af-80aa-4fcb4e6ede8b",
   "metadata": {},
   "source": [
    "## Optimizer Template for Simple Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5359d64-6788-4b73-8fed-75f90d283c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opto import trace\n",
    "from opto.trace import node, bundle\n",
    "from opto.optimizers.optoprime_v2 import OptoPrimeV2, OptimizerPromptSymbolSet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d1820a0d291c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = node(1, trainable=True)\n",
    "num_2 = node(2, trainable=True, description=\"<=5\")\n",
    "result = num_1 + num_2\n",
    "optimizer = OptoPrimeV2([num_1, num_2],use_json_object_format=False,\n",
    "                ignore_extraction_error=False,\n",
    "                include_example=True,\n",
    "                optimizer_prompt_symbol_set=OptimizerPromptSymbolSet2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c45a6a-41f6-4ac2-9591-8a331aca6e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809c46c6-c167-4d53-a8f9-e49dc56dd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_feedback()\n",
    "optimizer.backward(result, 'make this number bigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f02620-ef34-43da-8619-d1d07b4a4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = optimizer.summarize()\n",
    "part1, part2 = optimizer.construct_prompt(summary)\n",
    "\n",
    "part1 = optimizer.replace_symbols(part1, optimizer.prompt_symbols)\n",
    "part2 = optimizer.replace_symbols(part2, optimizer.prompt_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72494ea-550e-4d97-a9af-a520b3e854c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionFeedback(graph=[(1, 'add0 = add(x=int0, y=int1)')], documentation={'add': 'This is an add operator of x and y.'}, others={}, roots={'int0': (1, None), 'int1': (2, '<=5')}, output={'add0': (3, 'This is an add operator of x and y.')}, user_feedback='make this number bigger')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797944a6-46c2-45b9-9844-caed54319d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In `#Variables`, `#Inputs`, `#Outputs`, and `#Others`, the format is:\n",
      "\n",
      "For variables we express as this:\n",
      "\n",
      "<var name=\"variable_name\" type=\"data_type\">\n",
      "<data>\n",
      "value\n",
      "</data>\n",
      "<constraint>\n",
      "constraint_expression\n",
      "</constraint>\n",
      "</var>\n",
      "\n",
      "\n",
      "If `data_type` is `code`, it means `data` is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following XML/HTML format:\n",
      "\n",
      "```\n",
      "\n",
      "<reason>\n",
      "reasoning\n",
      "</reason>\n",
      "<var>\n",
      "<name>variable_name</name>\n",
      "<data>\n",
      "value\n",
      "</data>\n",
      "</var>\n",
      "\n",
      "```\n",
      "\n",
      "In <reason>, explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Outputs means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in <var>. Remember you can change only the values in #Variables, not others. When `type` of a variable is `code`, you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes are needed, just output TERMINATE.\n",
      "\n"
     ]
    }
   ],
   "source": "print(part1)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce1ae12-4fc7-4b6e-a79b-ea839bd007cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is an example of problem instance and response:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `data` of the variables in # Variables to improve the output in accordance to # Feedback.\n",
      "\n",
      "# Code\n",
      "y = add(x=a,y=b)\n",
      "z = subtract(x=y, y=c)\n",
      "\n",
      "# Documentation\n",
      "[add] This is an add operator of x and y.\n",
      "[subtract] subtract y from x\n",
      "\n",
      "# Variables\n",
      "<var name=\"a\" type=\"int\">\n",
      "<data>\n",
      "5\n",
      "</data>\n",
      "<constraint>\n",
      "a > 0\n",
      "</constraint>\n",
      "</var>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "<const name=\"b\" type=\"int\">\n",
      "<data>\n",
      "1\n",
      "</data>\n",
      "</const>\n",
      "\n",
      "<const name=\"c\" type=\"int\">\n",
      "<data>\n",
      "5\n",
      "</data>\n",
      "</const>\n",
      "\n",
      "\n",
      "# Others\n",
      "<const name=\"y\" type=\"int\">\n",
      "<data>\n",
      "6\n",
      "</data>\n",
      "</const>\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<const name=\"z\" type=\"int\">\n",
      "<data>\n",
      "1\n",
      "</data>\n",
      "</const>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "The result of the code is not as expected. The result should be 10, but the code returns 1\n",
      "\n",
      "================================\n",
      "\n",
      "Your response:\n",
      "\n",
      "<reason>\n",
      "In this case, the desired response would be to change the value of input a to 14, as that would make the code return 10.\n",
      "</reason>\n",
      "\n",
      "<var>\n",
      "<name>a</name>\n",
      "<data>\n",
      "10\n",
      "</data>\n",
      "</var>\n",
      "\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `data` of the variables in # Variables to improve the output in accordance to # Feedback.\n",
      "\n",
      "# Code\n",
      "add0 = add(x=int0, y=int1)\n",
      "\n",
      "# Documentation\n",
      "[add] This is an add operator of x and y.\n",
      "\n",
      "# Variables\n",
      "<var name=\"int0\" type=\"int\">\n",
      "<data>\n",
      "1\n",
      "</data>\n",
      "</var>\n",
      "\n",
      "<var name=\"int1\" type=\"int\">\n",
      "<data>\n",
      "2\n",
      "</data>\n",
      "<constraint>\n",
      "<=5\n",
      "</constraint>\n",
      "</var>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "\n",
      "\n",
      "# Others\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<const name=\"add0\" type=\"int\">\n",
      "<data>\n",
      "3\n",
      "</data>\n",
      "</const>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "make this number bigger\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "What are your suggestions on variables int0, int1?\n",
      "\n",
      "Your response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd0a50a-e691-46ec-930d-ac66498a001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": part1},\n",
    "    {\"role\": \"user\", \"content\": part2},\n",
    "]\n",
    "\n",
    "response = optimizer.llm(messages=messages)\n",
    "response = response.choices[0].message.content\n",
    "reasoning = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ac13cc8-805e-4038-9fba-32f27af23fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "<reason>\n",
      "The instruction suggests that the output, `add0`, needs to be made bigger than it currently is (3). The code performs an addition of `int0` and `int1` to produce `add0`. To increase `add0`, we can increase the values of `int0` or `int1`, or both. Given that `int1` has a constraint of being less than or equal to 5, we can set `int0` to a higher value, since it has no explicit constraint. By adjusting `int0` to a higher value, the output can be made larger in accordance with the feedback.\n",
      "</reason>\n",
      "\n",
      "<var>\n",
      "<name>int0</name>\n",
      "<data>\n",
      "5\n",
      "</data>\n",
      "</var>\n",
      "\n",
      "<var>\n",
      "<name>int1</name>\n",
      "<data>\n",
      "5\n",
      "</data>\n",
      "</var>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ce1b76-602d-4e76-afdf-6afdc676632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion = optimizer.extract_llm_suggestion(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8617b87-4658-49a4-b288-b64fd9e2b362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The instruction suggests that the output, `add0`, needs to be made bigger than it currently is (3). The code performs an addition of `int0` and `int1` to produce `add0`. To increase `add0`, we can increase the values of `int0` or `int1`, or both. Given that `int1` has a constraint of being less than or equal to 5, we can set `int0` to a higher value, since it has no explicit constraint. By adjusting `int0` to a higher value, the output can be made larger in accordance with the feedback.',\n",
       " 'variables': {'int0': '5', 'int1': '5'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8182a-642c-4b88-bc3e-46b2ef2d8bc7",
   "metadata": {},
   "source": [
    "## Optimizer template for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c178ba8-3eaa-436a-985b-806b8ef8323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opto import trace\n",
    "from opto.trace import node, bundle\n",
    "from opto.optimizers.optoprime_v2 import OptoPrimeV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "431c772c-051d-4dac-8874-91be21a02bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = node(1, trainable=False)\n",
    "\n",
    "@bundle()\n",
    "def transform(num):\n",
    "    \"\"\"Add number\"\"\"\n",
    "    return num+1\n",
    "\n",
    "@bundle(trainable=True)\n",
    "def multiply(num):\n",
    "    return num * 5\n",
    "\n",
    "result = multiply(transform(num_1))\n",
    "optimizer = OptoPrimeV2([multiply.parameter],use_json_object_format=False,\n",
    "                ignore_extraction_error=False,\n",
    "                include_example=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5b4ee88-5515-45b9-88eb-f48407a85305",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_feedback()\n",
    "optimizer.backward(result, 'make this number bigger')\n",
    "\n",
    "summary = optimizer.summarize()\n",
    "part1, part2 = optimizer.construct_prompt(summary)\n",
    "\n",
    "part1 = optimizer.replace_symbols(part1, optimizer.prompt_symbols)\n",
    "part2 = optimizer.replace_symbols(part2, optimizer.prompt_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41d2494-43a2-437e-869b-c459cd63122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In `#Variables`, `#Inputs`, `#Outputs`, and `#Others`, the format is:\n",
      "\n",
      "For variables we express as this:\n",
      "\n",
      "<variable name=\"variable_name\" type=\"data_type\">\n",
      "<value>\n",
      "value\n",
      "</value>\n",
      "<constraint>\n",
      "constraint_expression\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "\n",
      "If `data_type` is `code`, it means `value` is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following XML/HTML format:\n",
      "\n",
      "```\n",
      "\n",
      "<reasoning>\n",
      "reasoning\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>variable_name</name>\n",
      "<value>\n",
      "value\n",
      "</value>\n",
      "</variable>\n",
      "\n",
      "```\n",
      "\n",
      "In <reasoning>, explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Outputs means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in <variable>. Remember you can change only the values in #Variables, not others. When `type` of a variable is `code`, you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes are needed, just output TERMINATE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa754d0-af6a-43b1-8b87-13a1ea5c616a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is an example of problem instance and response:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `value` of the variables in # Variables to improve the output in accordance to # Feedback.\n",
      "\n",
      "# Code\n",
      "y = add(x=a,y=b)\n",
      "z = subtract(x=y, y=c)\n",
      "\n",
      "# Documentation\n",
      "[add] This is an add operator of x and y.\n",
      "[subtract] subtract y from x\n",
      "\n",
      "# Variables\n",
      "<variable name=\"a\" type=\"int\">\n",
      "<value>\n",
      "5\n",
      "</value>\n",
      "<constraint>\n",
      "a > 0\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "<node name=\"b\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "<node name=\"c\" type=\"int\">\n",
      "<value>\n",
      "5\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Others\n",
      "<node name=\"y\" type=\"int\">\n",
      "<value>\n",
      "6\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<node name=\"z\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "The result of the code is not as expected. The result should be 10, but the code returns 1\n",
      "\n",
      "================================\n",
      "\n",
      "Your response:\n",
      "\n",
      "<reasoning>\n",
      "In this case, the desired response would be to change the value of input a to 14, as that would make the code return 10.\n",
      "</reasoning>\n",
      "\n",
      "<variable>\n",
      "<name>a</name>\n",
      "<value>\n",
      "10\n",
      "</value>\n",
      "</variable>\n",
      "\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `value` of the variables in # Variables to improve the output in accordance to # Feedback.\n",
      "\n",
      "# Code\n",
      "eval0 = eval(num=transform0, __code=__code0)\n",
      "\n",
      "# Documentation\n",
      "[eval] This operator eval(__code, *args, **kwargs) evaluates the code block, where __code is the code (str) and *args and **kwargs are the arguments of the function. The output is the result of the evaluation, i.e., __code(*args, **kwargs).\n",
      "\n",
      "# Variables\n",
      "<variable name=\"__code0\" type=\"code\">\n",
      "<value>\n",
      "def multiply(num):\n",
      "    return num * 5\n",
      "</value>\n",
      "<constraint>\n",
      "The code should start with:\n",
      "def multiply(num):\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "<node name=\"transform0\" type=\"int\">\n",
      "<value>\n",
      "2\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Others\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<node name=\"eval0\" type=\"int\">\n",
      "<value>\n",
      "10\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "make this number bigger\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "What are your suggestions on variables __code0?\n",
      "\n",
      "Your response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All of the XML tags are things people can change\n",
    "# All of section titles\n",
    "\n",
    "print(part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e89b1ea-bb87-4e43-9a6d-e5be80cc6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": part1},\n",
    "    {\"role\": \"user\", \"content\": part2},\n",
    "]\n",
    "\n",
    "response = optimizer.llm(messages=messages)\n",
    "response = response.choices[0].message.content\n",
    "reasoning = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80198862-2c0c-4967-8536-12469e77b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "The `eval` function is being used to evaluate `__code0` using `transform0` as an argument. In the current code, `multiply(num)` returns `num * 5`. Given the `transform0` input as 2, the current execution returns `eval0 = 2 * 5 = 10`.\n",
      "\n",
      "The feedback suggests \"make this number bigger,\" meaning `eval0` should be larger than 10. To achieve this, we need to change the implementation of the `multiply` function in `__code0`. One way is to increase the multiplier factor or add a constant term to the result to achieve a number greater than 10.\n",
      "\n",
      "To address the feedback, we could adjust the `multiply` function in `__code0` to return `num * 10` or any other larger multiplier based on the condition `eval0 > 10`.\n",
      "\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>__code0</name>\n",
      "<value>\n",
      "def multiply(num):\n",
      "    return num * 10\n",
      "</value>\n",
      "</variable>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b895f49-9458-4e82-97bc-24ae4ec508d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion = optimizer.extract_llm_suggestion(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47f0e0ac-77d0-466f-9d0e-d73cdd1ea942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The `eval` function is being used to evaluate `__code0` using `transform0` as an argument. In the current code, `multiply(num)` returns `num * 5`. Given the `transform0` input as 2, the current execution returns `eval0 = 2 * 5 = 10`.\\n\\nThe feedback suggests \"make this number bigger,\" meaning `eval0` should be larger than 10. To achieve this, we need to change the implementation of the `multiply` function in `__code0`. One way is to increase the multiplier factor or add a constant term to the result to achieve a number greater than 10.\\n\\nTo address the feedback, we could adjust the `multiply` function in `__code0` to return `num * 10` or any other larger multiplier based on the condition `eval0 > 10`.',\n",
       " 'variables': {'__code0': 'def multiply(num):\\n    return num * 10'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a33a0-1733-4e20-9d18-e2ab956ee3df",
   "metadata": {},
   "source": [
    "## Optimizer template for list, dict, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70554c88-ce8b-4965-b867-669d7110715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opto import trace\n",
    "from opto.trace import node, bundle\n",
    "from opto.optimizers.optoprime_v2 import OptoPrimeV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a343ff-257a-4997-a12b-dc0b24239c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = node(1, trainable=True)\n",
    "\n",
    "list_1 = node([1,2,3,4,5,6,7,8,9,20] * 10, trainable=True)\n",
    "\n",
    "result = num_1 + list_1[30]\n",
    "\n",
    "optimizer = OptoPrimeV2([num_1, list_1],use_json_object_format=False,\n",
    "                ignore_extraction_error=False,\n",
    "                include_example=True, initial_var_char_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a56a1a7-3455-43fb-a13e-bf4a7462efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_feedback()\n",
    "optimizer.backward(result, 'make this number bigger')\n",
    "\n",
    "summary = optimizer.summarize()\n",
    "part1, part2 = optimizer.construct_prompt(summary)\n",
    "\n",
    "part1 = optimizer.replace_symbols(part1, optimizer.prompt_symbols)\n",
    "part2 = optimizer.replace_symbols(part2, optimizer.prompt_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f180377-9ddb-4cde-8b8b-0cb6f390aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is an example of problem instance and response:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `value` of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "# Code\n",
      "y = add(x=a,y=b)\n",
      "z = subtract(x=y, y=c)\n",
      "\n",
      "# Documentation\n",
      "add: add x and y \n",
      "subtract: subtract y from x\n",
      "\n",
      "# Variables\n",
      "<variable name=\"a\" type=\"int\">\n",
      "<value>\n",
      "5\n",
      "</value>\n",
      "<constraint>\n",
      "a: a > 0\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "# Inputs\n",
      "<node name=\"b\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "<node name=\"c\" type=\"int\">\n",
      "<value>\n",
      "5\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "# Others\n",
      "<node name=\"y\" type=\"int\">\n",
      "<value>\n",
      "6\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "# Outputs\n",
      "<node name=\"z\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "# Feedback\n",
      "The result of the code is not as expected. The result should be 10, but the code returns 1\n",
      "\n",
      "================================\n",
      "\n",
      "Your response:\n",
      "\n",
      "<reasoning>\n",
      "In this case, the desired response would be to change the value of input a to 14, as that would make the code return 10.\n",
      "</reasoning>\n",
      "\n",
      "<variable>\n",
      "<name>a</name>\n",
      "<value>\n",
      "10\n",
      "</value>\n",
      "</variable>\n",
      "\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `value` of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "# Code\n",
      "getitem0 = getitem(x=list0, index=int3)\n",
      "add1 = add(x=int2, y=getitem0)\n",
      "\n",
      "# Documentation\n",
      "[getitem] This is a getitem operator of x based on index.\n",
      "[add] This is an add operator of x and y.\n",
      "\n",
      "# Variables\n",
      "<variable name=\"int2\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</variable>\n",
      "\n",
      "<variable name=\"list0\" type=\"list\">\n",
      "<value>\n",
      "[1, 2, 3, ...(skipped due to length limit)\n",
      "</value>\n",
      "</variable>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "<node name=\"int3\" type=\"int\">\n",
      "<value>\n",
      "30\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Others\n",
      "<node name=\"getitem0\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<node name=\"add1\" type=\"int\">\n",
      "<value>\n",
      "2\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "make this number bigger\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "What are your suggestions on variables int2, list0?\n",
      "\n",
      "Your response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a05b07-dcd7-4871-9eb7-e39480043ca1",
   "metadata": {},
   "source": [
    "## Test JSON Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1890417f-1543-49b3-bd07-1e3c8b015c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opto import trace\n",
    "from opto.trace import node, bundle\n",
    "from opto.optimizers.optoprime_v2 import OptoPrimeV2, OptimizerPromptSymbolSetJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4246e07b-ea23-4cdf-b76f-be1feaa2b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = node(1, trainable=False)\n",
    "\n",
    "@bundle()\n",
    "def transform(num):\n",
    "    \"\"\"Add number\"\"\"\n",
    "    return num+1\n",
    "\n",
    "@bundle(trainable=True)\n",
    "def multiply(num):\n",
    "    return num * 5\n",
    "\n",
    "result = multiply(transform(num_1))\n",
    "optimizer = OptoPrimeV2([multiply.parameter],use_json_object_format=False,\n",
    "                ignore_extraction_error=False,\n",
    "                include_example=True,\n",
    "                optimizer_prompt_symbol_set=OptimizerPromptSymbolSetJSON())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4950df-11a1-4f44-a955-a69eeb3b9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_feedback()\n",
    "optimizer.backward(result, 'make this number bigger')\n",
    "\n",
    "summary = optimizer.summarize()\n",
    "part1, part2 = optimizer.construct_prompt(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321d0310-dc0a-4ba3-b70b-c28902834fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change/tweak (trainable).\n",
      "- #Inputs: the values of fixed inputs to the code, which CANNOT be changed (fixed).\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In `#Variables`, `#Inputs`, `#Outputs`, and `#Others`, the format is:\n",
      "\n",
      "For variables we express as this:\n",
      "\n",
      "<variable name=\"variable_name\" type=\"data_type\">\n",
      "<value>\n",
      "value\n",
      "</value>\n",
      "<constraint>\n",
      "constraint_expression\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "\n",
      "If `data_type` is `code`, it means `value` is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following XML/HTML format:\n",
      "\n",
      "```\n",
      "{{\n",
      "        \"reasoning\": <Your reasoning>,\n",
      "        \"suggestion\": {{\n",
      "            <variable_1>: <suggested_value_1>,\n",
      "            <variable_2>: <suggested_value_2>,\n",
      "        }}\n",
      "    }}\n",
      "```\n",
      "\n",
      "In <reasoning>, explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Outputs means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in <variable>. Remember you can change only the values in #Variables, not others. When `type` of a variable is `code`, you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes are needed, just output TERMINATE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3ff2a3-254d-4dc9-aa73-eb5d3aad1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is an example of problem instance and response:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `value` of the variables in # Variables to improve the output in accordance to # Feedback.\n",
      "\n",
      "# Code\n",
      "y = add(x=a,y=b)\n",
      "z = subtract(x=y, y=c)\n",
      "\n",
      "# Documentation\n",
      "[add] This is an add operator of x and y.\n",
      "[subtract] subtract y from x\n",
      "\n",
      "# Variables\n",
      "<variable name=\"a\" type=\"int\">\n",
      "<value>\n",
      "5\n",
      "</value>\n",
      "<constraint>\n",
      "a > 0\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "<node name=\"b\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "<node name=\"c\" type=\"int\">\n",
      "<value>\n",
      "5\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Others\n",
      "<node name=\"y\" type=\"int\">\n",
      "<value>\n",
      "6\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<node name=\"z\" type=\"int\">\n",
      "<value>\n",
      "1\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "The result of the code is not as expected. The result should be 10, but the code returns 1\n",
      "\n",
      "================================\n",
      "\n",
      "Your response:\n",
      "{\n",
      "  \"reasoning\": \"In this case, the desired response would be to change the value of input a to 14, as that would make the code return 10.\",\n",
      "  \"suggestion\": {\n",
      "    \"a\": 10\n",
      "  }\n",
      "}\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "# Instruction\n",
      "You need to change the `value` of the variables in # Variables to improve the output in accordance to # Feedback.\n",
      "\n",
      "# Code\n",
      "eval0 = eval(num=transform0, __code=__code0)\n",
      "\n",
      "# Documentation\n",
      "[eval] This operator eval(__code, *args, **kwargs) evaluates the code block, where __code is the code (str) and *args and **kwargs are the arguments of the function. The output is the result of the evaluation, i.e., __code(*args, **kwargs).\n",
      "\n",
      "# Variables\n",
      "<variable name=\"__code0\" type=\"code\">\n",
      "<value>\n",
      "def multiply(num):\n",
      "    return num * 5\n",
      "</value>\n",
      "<constraint>\n",
      "The code should start with:\n",
      "def multiply(num):\n",
      "</constraint>\n",
      "</variable>\n",
      "\n",
      "\n",
      "# Inputs\n",
      "<node name=\"transform0\" type=\"int\">\n",
      "<value>\n",
      "2\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Others\n",
      "\n",
      "\n",
      "# Outputs\n",
      "<node name=\"eval0\" type=\"int\">\n",
      "<value>\n",
      "10\n",
      "</value>\n",
      "</node>\n",
      "\n",
      "\n",
      "# Feedback\n",
      "make this number bigger\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "What are your suggestions on variables __code0?\n",
      "\n",
      "Your response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4e7b87-60c7-4210-ba6e-8c3061ac83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": part1},\n",
    "    {\"role\": \"user\", \"content\": part2},\n",
    "]\n",
    "\n",
    "response = optimizer.llm(messages=messages)\n",
    "response = response.choices[0].message.content\n",
    "reasoning = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce01a48-49f5-4413-afac-0753674157a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"reasoning\": \"The instruction indicates that the output should be greater than the current output of 10. The function `multiply(num)` multiplies its input `num` by 5. Given that `transform0` is 2, the function correctly calculates 2 * 5 = 10. To increase the output, the multiplication factor in the function needs to be increased. Thus, modifying the function in `__code0` to return `num * 7` (for example) will increase the output, making it 14.\",\n",
      "    \"suggestion\": {\n",
      "        \"__code0\": \"def multiply(num):\\n    return num * 7\"\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4b9857-e558-43e4-8c26-1bb2cde79d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion = optimizer.extract_llm_suggestion(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e2a87a-fcae-4037-b817-74e26fc139e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The instruction indicates that the output should be greater than the current output of 10. The function `multiply(num)` multiplies its input `num` by 5. Given that `transform0` is 2, the function correctly calculates 2 * 5 = 10. To increase the output, the multiplication factor in the function needs to be increased. Thus, modifying the function in `__code0` to return `num * 7` (for example) will increase the output, making it 14.',\n",
       " 'variables': {'__code0': 'def multiply(num):\\n    return num * 7'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc3c9-cd06-42e3-80a0-0f64434e8ac9",
   "metadata": {},
   "source": [
    "# Integrated search capabilities\n",
    "\n",
    "- Build a prompt demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5e9365-9f3a-47e2-85e9-b1153557be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opto.trainer.algorithms.basic_algorithms import MinibatchAlgorithm, BasicSearchAlgorithm\n",
    "from opto.trainer.algorithms.beamsearch_algorithm import BeamsearchAlgorithm, BeamsearchHistoryAlgorithm\n",
    "from opto.trainer.algorithms.UCBsearch import UCBSearchAlgorithm\n",
    "from opto.trainer.guide import AutoGuide\n",
    "\n",
    "from opto.optimizers.optoprime_v2 import OptoPrimeV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96b1e78-c1b4-4629-89ee-b5ea03959c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opto import trace\n",
    "from opto.utils.llm import LLM\n",
    "from typing import Any, Tuple\n",
    "\n",
    "from opto.trainer.loggers import DefaultLogger\n",
    "\n",
    "class SimpleLogger(DefaultLogger):\n",
    "    \"\"\"Simplified logger that only shows important metrics.\"\"\"\n",
    "    \n",
    "    def log(self, name: str, data: Any, step: int, **kwargs):\n",
    "        \"\"\"Log only specific metrics to reduce output clutter.\n",
    "        \n",
    "        Args:\n",
    "            name: The name of the metric\n",
    "            data: The metric value\n",
    "            step: The current step\n",
    "            **kwargs: Additional logging arguments\n",
    "        \"\"\"\n",
    "        important_metrics = [\n",
    "            'Average train score',\n",
    "            'Average test score',\n",
    "            'Validation score'\n",
    "        ]\n",
    "        \n",
    "        if name in important_metrics or 'Parameter' in name:\n",
    "            super().log(name, data, step, **kwargs)\n",
    "\n",
    "@trace.model\n",
    "class Learner(trace.Module):\n",
    "    \"\"\"A basic LLM Agent for solving math problems.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                system_prompt: str = \"You're a helpful agent answering math problems.\",\n",
    "                user_prompt_template: str = \"Solve the following math problem step-by-step: {message}\",\n",
    "                llm: LLM = None):\n",
    "        \"\"\"Initialize the learner agent.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System prompt to guide LLM behavior\n",
    "            user_prompt_template: Template for formatting user messages\n",
    "            llm: LLM instance to use for generation (defaults to gpt-3.5-turbo)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.system_prompt = trace.node(system_prompt, trainable=True)\n",
    "        self.user_prompt_template = trace.node(user_prompt_template, trainable=True)\n",
    "        self.llm = llm or LLM(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    @trace.bundle()\n",
    "    def call_llm(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"Call LLM model with the given prompts.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: The system prompt\n",
    "            user_prompt: The user prompt\n",
    "            \n",
    "        Returns:\n",
    "            The LLM response content\n",
    "        \"\"\"\n",
    "        response = self.llm(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def forward(self, message: Any) -> str:\n",
    "        \"\"\"Agent's forward pass to process a message.\n",
    "        \n",
    "        Args:\n",
    "            message: The input message to process\n",
    "            \n",
    "        Returns:\n",
    "            The generated response\n",
    "        \"\"\" \n",
    "        user_prompt = self.user_prompt_template.format(message=message)\n",
    "        return self.call_llm(self.system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "\n",
    "class TeacherGuide(AutoGuide):\n",
    "    \"\"\"Guide that uses LLM to judge answers and provide feedback.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"Initialize the teacher guide.\n",
    "        \n",
    "        Args:\n",
    "            model: The LLM model to use for evaluation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.guide_llm = LLM(model=model)\n",
    "        self.system_prompt = \"You are an expert math teacher evaluating student answers.\"\n",
    "        self.judge_prompt_template = (\n",
    "            \"Carefully review the following three distinct sections:\\n\\n\"\n",
    "            \"SECTION 1: The Math Problem\\n\"\n",
    "            \"----------------------------\\n\"\n",
    "            \"{query}\\n\"\n",
    "            \"----------------------------\\n\\n\"\n",
    "            \"SECTION 2: The Student's Full Answer\\n\"\n",
    "            \"----------------------------\\n\"\n",
    "            \"{response}\\n\"\n",
    "            \"----------------------------\\n\\n\"\n",
    "            \"SECTION 3: The Official Correct Answer\\n\"\n",
    "            \"----------------------------\\n\"\n",
    "            \"{reference}\\n\"\n",
    "            \"----------------------------\\n\\n\"\n",
    "            \"INSTRUCTIONS FOR JUDGING:\\n\"\n",
    "            \"1. Your primary task is to compare the student's **final numerical result** (or final conclusion if no number is present) from SECTION 2 with the **Official Correct Answer** provided in SECTION 3.\\n\"\n",
    "            \"2. When evaluating SECTION 2 (Student's Full Answer), focus SOLELY on the **final answer part** of the student's response. Ignore all intermediate steps, reasoning, or explanations for the correctness check unless the problem specifically asks for reasoning as the final answer.\\n\"\n",
    "            \"3. Determine if the student's **final answer** is equivalent to the **Official Correct Answer**.\\n\\n\"\n",
    "            \"RESPONSE FORMAT:\\n\"\n",
    "            \"- If the student's final answer (from SECTION 2) IS equivalent to the Official Correct Answer (from SECTION 3), respond ONLY with the exact phrase: 'Correct [TERMINATE]'\\n\"\n",
    "            \"- If the student's final answer IS NOT equivalent, respond ONLY with specific and actionable feedback. The feedback should clearly explain the error in the student's final answer and guide them on how to arrive at the Official Correct Answer.\"\n",
    "        )\n",
    "\n",
    "    def get_feedback(self, task: str, response: str, info: Any, **kwargs) -> Tuple[float, str]:\n",
    "        \"\"\"Get feedback on a student response.\n",
    "        \n",
    "        Args:\n",
    "            task: The original math problem\n",
    "            response: The student's answer\n",
    "            info: The reference/correct answer\n",
    "            **kwargs: Additional arguments\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (score, feedback_text)\n",
    "        \"\"\"\n",
    "        user_prompt = self.judge_prompt_template.format(\n",
    "            query=task,\n",
    "            response=response,\n",
    "            reference=info\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        llm_response = self.guide_llm(messages=messages)\n",
    "        feedback_text = llm_response.choices[0].message.content\n",
    "\n",
    "        if 'Correct [TERMINATE]' in feedback_text:\n",
    "            return 1.0, \"Correct.\"\n",
    "        else:\n",
    "            return 0.0, f\"Incorrect. Feedback: {feedback_text}\"\n",
    "    \n",
    "    def metric(self, task: str, content: str, info: Any, **kwargs) -> float:\n",
    "        \"\"\"Calculate the metric score for an answer.\n",
    "        \n",
    "        Args:\n",
    "            task: The original math problem\n",
    "            content: The student's answer\n",
    "            info: The reference/correct answer\n",
    "            **kwargs: Additional arguments\n",
    "            \n",
    "        Returns:\n",
    "            Score (0.0 or 1.0)\n",
    "        \"\"\"\n",
    "        score, _ = self.get_feedback(task, content, info, **kwargs)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ffec18ff-a6f2-4622-aa93-9a142efbd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, List, Dict, Union, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from opto.optimizers.optoprime import OptoPrime, FunctionFeedback\n",
    "from opto.trace.utils import dedent\n",
    "\n",
    "from opto.trace.nodes import ParameterNode, Node, MessageNode\n",
    "from opto.trace.propagators import TraceGraph, GraphPropagator\n",
    "from opto.trace.propagators.propagators import Propagator\n",
    "\n",
    "from opto.utils.llm import AbstractModel, LLM\n",
    "from opto.optimizers.buffers import FIFOBuffer\n",
    "import copy\n",
    "\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def extract_top_level_blocks(text: str, tag: str):\n",
    "    \"\"\"Extract all top-level <tag>...</tag> blocks from text.\"\"\"\n",
    "    blocks = []\n",
    "    start_tag = f'<{tag}>'\n",
    "    end_tag = f'</{tag}>'\n",
    "    stack = []\n",
    "    start = None\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        if text.startswith(start_tag, i):\n",
    "            if not stack:\n",
    "                start = i + len(start_tag)\n",
    "            stack.append(i)\n",
    "            i += len(start_tag)\n",
    "        elif text.startswith(end_tag, i):\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack and start is not None:\n",
    "                    blocks.append(text[start:i])\n",
    "                    start = None\n",
    "            i += len(end_tag)\n",
    "        else:\n",
    "            i += 1\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def extract_first_top_level_block(text: str, tag: str):\n",
    "    blocks = extract_top_level_blocks(text, tag)\n",
    "    return blocks[0] if blocks else None\n",
    "\n",
    "\n",
    "def strip_nested_blocks(text: str, tag: str) -> str:\n",
    "    \"\"\"Remove all nested <tag>...</tag> blocks from text, leaving only the top-level text.\"\"\"\n",
    "    result = ''\n",
    "    start_tag = f'<{tag}>'\n",
    "    end_tag = f'</{tag}>'\n",
    "    stack = []\n",
    "    i = 0\n",
    "    last = 0\n",
    "    while i < len(text):\n",
    "        if text.startswith(start_tag, i):\n",
    "            if not stack:\n",
    "                result += text[last:i]\n",
    "            stack.append(i)\n",
    "            i += len(start_tag)\n",
    "        elif text.startswith(end_tag, i):\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    last = i + len(end_tag)\n",
    "            i += len(end_tag)\n",
    "        else:\n",
    "            i += 1\n",
    "    if not stack:\n",
    "        result += text[last:]\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "def extract_reasoning_and_remainder(text: str, tag: str = \"reasoning\"):\n",
    "    \"\"\"Extract reasoning and the remainder of the text after reasoning block (if closed). Strip whitespace only if properly closed.\"\"\"\n",
    "    start_tag = f'<{tag}>'\n",
    "    end_tag = f'</{tag}>'\n",
    "    start = text.find(start_tag)\n",
    "    if start == -1:\n",
    "        return '', text\n",
    "    start += len(start_tag)\n",
    "    end = text.find(end_tag, start)\n",
    "    if end == -1:\n",
    "        # If not properly closed, don't strip whitespace to preserve original formatting\n",
    "        return text[start:], ''\n",
    "    return text[start:end].strip(), text[end + len(end_tag):]\n",
    "\n",
    "\n",
    "def extract_xml_like_data(text: str, reasoning_tag: str = \"reasoning\",\n",
    "                          improved_variable_tag: str = \"variable\",\n",
    "                          name_tag: str = \"name\",\n",
    "                          value_tag: str = \"value\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract thinking content and improved variables from text containing XML-like tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text containing <reasoning> and <variable> tags\n",
    "\n",
    "    Returns:\n",
    "        Dict containing:\n",
    "        - 'reasoning': content of <reasoning> element\n",
    "        - 'variables': dict mapping variable names to their values\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'reasoning': '',\n",
    "        'variables': {}\n",
    "    }\n",
    "\n",
    "    # Extract reasoning and the remainder of the text\n",
    "    reasoning, remainder = extract_reasoning_and_remainder(text, reasoning_tag)\n",
    "    result['reasoning'] = reasoning\n",
    "\n",
    "    # Only parse variables from the remainder (i.e., after a closed reasoning tag)\n",
    "    variable_blocks = extract_top_level_blocks(remainder, improved_variable_tag)\n",
    "    for var_block in variable_blocks:\n",
    "        name_block = extract_first_top_level_block(var_block, name_tag)\n",
    "        value_block = extract_first_top_level_block(var_block, value_tag)\n",
    "        # Only add if both name and value tags are present and name is non-empty after stripping\n",
    "        if name_block is not None and value_block is not None:\n",
    "            var_name = name_block.strip()\n",
    "            var_value = value_block.strip() if value_block is not None else ''\n",
    "            if var_name:  # Only require name to be non-empty, value can be empty\n",
    "                result['variables'][var_name] = var_value\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class OptimizerPromptSymbolSet:\n",
    "    \"\"\"\n",
    "    By inheriting this class and pass into the optimizer. People can change the optimizer documentation\n",
    "\n",
    "    This divides into three parts:\n",
    "    - Section titles: the title of each section in the prompt\n",
    "    - Node tags: the tags that capture the graph structure (only tag names are allowed to be changed)\n",
    "    - Output format: the format of the output of the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    variables_section_title = \"# Variables\"\n",
    "    inputs_section_title = \"# Inputs\"\n",
    "    outputs_section_title = \"# Outputs\"\n",
    "    others_section_title = \"# Others\"\n",
    "    feedback_section_title = \"# Feedback\"\n",
    "    instruction_section_title = \"# Instruction\"\n",
    "    code_section_title = \"# Code\"\n",
    "    documentation_section_title = \"# Documentation\"\n",
    "\n",
    "    node_tag = \"node\"  # nodes that are constants in the graph\n",
    "    variable_tag = \"variable\"  # nodes that can be changed\n",
    "    value_tag = \"value\"  # inside node, we have value tag\n",
    "    constraint_tag = \"constraint\"  # inside node, we have constraint tag\n",
    "\n",
    "    # output format\n",
    "    # Note: we currently don't support extracting format's like \"```code```\" because we assume supplied tag is name-only, i.e., <tag_name></tag_name>\n",
    "    reasoning_tag = \"reasoning\"\n",
    "    improved_variable_tag = \"variable\"\n",
    "    name_tag = \"name\"\n",
    "\n",
    "    # custom output format (this will give the highest degree of freedom)\n",
    "    # once it's set, it will override the default output format\n",
    "    output_format_prompt_instruction = None\n",
    "\n",
    "    def output_response_extractor(self, response: str) -> Dict[str, Any]:\n",
    "        if self.output_format_prompt_instruction is None:\n",
    "            extracted_data = extract_xml_like_data(response,\n",
    "                                                   reasoning_tag=self.reasoning_tag,\n",
    "                                                   improved_variable_tag=self.improved_variable_tag,\n",
    "                                                   name_tag=self.name_tag,\n",
    "                                                   value_tag=self.value_tag)\n",
    "            return extracted_data\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"If you supplied a custom output format prompt template, you need to implement your own response extractor\")\n",
    "        \n",
    "    @property\n",
    "    def default_prompt_symbols(self) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"variables\": self.variables_section_title,\n",
    "            \"inputs\": self.inputs_section_title,\n",
    "            \"outputs\": self.outputs_section_title,\n",
    "            \"others\": self.others_section_title,\n",
    "            \"feedback\": self.feedback_section_title,\n",
    "            \"instruction\": self.instruction_section_title,\n",
    "            \"code\": self.code_section_title,\n",
    "            \"documentation\": self.documentation_section_title,\n",
    "        }\n",
    "\n",
    "\n",
    "class OptimizerPromptSymbolSet2(OptimizerPromptSymbolSet):\n",
    "    variables_section_title = \"# Variables\"\n",
    "    inputs_section_title = \"# Inputs\"\n",
    "    outputs_section_title = \"# Outputs\"\n",
    "    others_section_title = \"# Others\"\n",
    "    feedback_section_title = \"# Feedback\"\n",
    "    instruction_section_title = \"# Instruction\"\n",
    "    code_section_title = \"# Code\"\n",
    "    documentation_section_title = \"# Documentation\"\n",
    "\n",
    "    node_tag = \"const\"  # nodes that are constants in the graph\n",
    "    variable_tag = \"var\"  # nodes that can be changed\n",
    "    value_tag = \"data\"  # inside node, we have value tag\n",
    "    constraint_tag = \"constraint\"  # inside node, we have constraint tag\n",
    "\n",
    "    # output format\n",
    "    reasoning_tag = \"reason\"\n",
    "    improved_variable_tag = \"var\"\n",
    "    name_tag = \"name\"\n",
    "    value_tag = \"data\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProblemInstance:\n",
    "    instruction: str\n",
    "    code: str\n",
    "    documentation: str\n",
    "    variables: str\n",
    "    inputs: str\n",
    "    others: str\n",
    "    outputs: str\n",
    "    feedback: str\n",
    "\n",
    "    optimizer_prompt_symbol_set: OptimizerPromptSymbolSet\n",
    "\n",
    "    problem_template = dedent(\n",
    "        \"\"\"\n",
    "        # Instruction\n",
    "        {instruction}\n",
    "\n",
    "        # Code\n",
    "        {code}\n",
    "\n",
    "        # Documentation\n",
    "        {documentation}\n",
    "\n",
    "        # Variables\n",
    "        {variables}\n",
    "\n",
    "        # Inputs\n",
    "        {inputs}\n",
    "\n",
    "        # Others\n",
    "        {others}\n",
    "\n",
    "        # Outputs\n",
    "        {outputs}\n",
    "\n",
    "        # Feedback\n",
    "        {feedback}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.replace_symbols(self.problem_template.format(\n",
    "            instruction=self.instruction,\n",
    "            code=self.code,\n",
    "            documentation=self.documentation,\n",
    "            variables=self.variables,\n",
    "            inputs=self.inputs,\n",
    "            outputs=self.outputs,\n",
    "            others=self.others,\n",
    "            feedback=self.feedback,\n",
    "        ), self.optimizer_prompt_symbol_set.default_prompt_symbols)\n",
    "    \n",
    "    def replace_symbols(self, text: str, symbols: Dict[str, str]) -> str:\n",
    "        default_prompt_symbols = {\n",
    "            \"variables\": \"# Variables\",\n",
    "            \"constraints\": \"# Constraints\",\n",
    "            \"inputs\": \"# Inputs\",\n",
    "            \"outputs\": \"# Outputs\",\n",
    "            \"others\": \"# Others\",\n",
    "            \"feedback\": \"# Feedback\",\n",
    "            \"instruction\": \"# Instruction\",\n",
    "            \"code\": \"# Code\",\n",
    "            \"documentation\": \"# Documentation\",\n",
    "        }\n",
    "            \n",
    "        for k, v in symbols.items():\n",
    "            text = text.replace(default_prompt_symbols[k], v)\n",
    "        return text\n",
    "\n",
    "\n",
    "# TODO: solution1 -> solution2 -> solution3\n",
    "# TODO: param(solution) optimzer.step(solution, \"reward is 1, maximize1) -> solution 2\n",
    "# TODO: maybe have a trace.train() # simpler even than Algorithm, and cover 80% of use cases\n",
    "\n",
    "class OptoPrimeV2(OptoPrime):\n",
    "    # TODO: LLM has the option to check the value of truncated one\n",
    "    # TODO: turn into a conversation round\n",
    "    # TODO: and show in a separate message\n",
    "    # TODO: 3. Compact representation (compress function)\n",
    "    # TODO: batchify, list of inputs, output is a list of inputs\n",
    "    # TODO: information is redundant\n",
    "    # TODO: idea 1: for each operator, we can identify repeated structure\n",
    "    # TODO: idea 2: for each bundle/op, the user can pass in a callable function, take original output, return a string\n",
    "    # TODO: idea 2-2: each node has a string representation of data, that's what the optimizer should use (this string is fixed)\n",
    "\n",
    "    # This is generic representation prompt, which just explains how to read the problem.\n",
    "    representation_prompt = dedent(\n",
    "        \"\"\"\n",
    "        You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
    "\n",
    "        Specifically, a problem will be composed of the following parts:\n",
    "        - {instruction_section_title}: the instruction which describes the things you need to do or the question you should answer.\n",
    "        - {code_section_title}: the code defined in the problem.\n",
    "        - {documentation_section_title}: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
    "        - {variables_section_title}: the input variables that you can change/tweak (trainable).\n",
    "        - {inputs_section_title}: the values of fixed inputs to the code, which CANNOT be changed (fixed).\n",
    "        - {others_section_title}: the intermediate values created through the code execution.\n",
    "        - {outputs_section_title}: the result of the code output.\n",
    "        - {feedback_section_title}: the feedback about the code's execution result.\n",
    "\n",
    "        In `{variables_section_title}`, `{inputs_section_title}`, `{outputs_section_title}`, and `{others_section_title}`, the format is:\n",
    "\n",
    "        For variables we express as this:\n",
    "        {variable_expression_format}\n",
    "        \n",
    "        If `data_type` is `code`, it means `{value_tag}` is the source code of a python code, which may include docstring and definitions.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Optimization\n",
    "    default_objective = \"You need to change the `{value_tag}` of the variables in {variables_section_title} to improve the output in accordance to {feedback_section_title}.\"\n",
    "\n",
    "    output_format_prompt_template = dedent(\n",
    "        \"\"\"\n",
    "        Output_format: Your output should be in the following XML/HTML format:\n",
    "        \n",
    "        ```\n",
    "        {output_format}\n",
    "        ```\n",
    "\n",
    "        In <{reasoning_tag}>, explain the problem: 1. what the {instruction_section_title} means 2. what the {feedback_section_title} on {outputs_section_title} means to {variables_section_title} considering how {variables_section_title} are used in {code_section_title} and other values in {documentation_section_title}, {inputs_section_title}, {others_section_title}. 3. Reasoning about the suggested changes in {variables_section_title} (if needed) and the expected result.\n",
    "\n",
    "        If you need to suggest a change in the values of {variables_section_title}, write down the suggested values in <{improved_variable_tag}>. Remember you can change only the values in {variables_section_title}, not others. When `type` of a variable is `code`, you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
    "\n",
    "        If no changes are needed, just output TERMINATE.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    example_problem_template = dedent(\n",
    "        \"\"\"\n",
    "        Here is an example of problem instance and response:\n",
    "\n",
    "        ================================\n",
    "        {example_problem}\n",
    "        ================================\n",
    "\n",
    "        Your response:\n",
    "        {example_response}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    user_prompt_template = dedent(\n",
    "        \"\"\"\n",
    "        Now you see problem instance:\n",
    "\n",
    "        ================================\n",
    "        {problem_instance}\n",
    "        ================================\n",
    "\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    example_prompt = dedent(\n",
    "        \"\"\"\n",
    "\n",
    "        Here are some feasible but not optimal solutions for the current problem instance. Consider this as a hint to help you understand the problem better.\n",
    "\n",
    "        ================================\n",
    "\n",
    "        {examples}\n",
    "\n",
    "        ================================\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    final_prompt = dedent(\n",
    "        \"\"\"\n",
    "        What are your suggestions on variables {names}?\n",
    "        \n",
    "        Your response:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            parameters: List[ParameterNode],\n",
    "            llm: AbstractModel = None,\n",
    "            *args,\n",
    "            propagator: Propagator = None,\n",
    "            objective: Union[None, str] = None,\n",
    "            ignore_extraction_error: bool = True,\n",
    "            # ignore the type conversion error when extracting updated values from LLM's suggestion\n",
    "            include_example=False,\n",
    "            memory_size=0,  # Memory size to store the past feedback\n",
    "            max_tokens=4096,\n",
    "            log=True,\n",
    "            initial_var_char_limit=100,\n",
    "            optimizer_prompt_symbol_set: OptimizerPromptSymbolSet = OptimizerPromptSymbolSet(),\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(parameters, *args, propagator=propagator, **kwargs)\n",
    "        self.ignore_extraction_error = ignore_extraction_error\n",
    "        self.llm = llm or LLM()\n",
    "        self.objective = objective or self.default_objective.format(value_tag=optimizer_prompt_symbol_set.value_tag,\n",
    "                                                                    variables_section_title=optimizer_prompt_symbol_set.variables_section_title,\n",
    "                                                                    feedback_section_title=optimizer_prompt_symbol_set.feedback_section_title)\n",
    "        self.initial_var_char_limit = initial_var_char_limit\n",
    "        self.optimizer_prompt_symbol_set = optimizer_prompt_symbol_set\n",
    "\n",
    "        self.example_problem_summary = FunctionFeedback(graph=[(1, 'y = add(x=a,y=b)'), (2, \"z = subtract(x=y, y=c)\")],\n",
    "                                                        documentation={'add': 'This is an add operator of x and y.',\n",
    "                                                                       'subtract': \"subtract y from x\"},\n",
    "                                                        others={'y': (6, None)},\n",
    "                                                        roots={'a': (5, \"a > 0\"),\n",
    "                                                               'b': (1, None),\n",
    "                                                               'c': (5, None)},\n",
    "                                                        output={'z': (1, None)},\n",
    "                                                        user_feedback='The result of the code is not as expected. The result should be 10, but the code returns 1'\n",
    "                                                        )\n",
    "        self.example_problem_summary.variables = {'a': (5, \"a > 0\")}\n",
    "        self.example_problem_summary.inputs = {'b': (1, None), 'c': (5, None)}\n",
    "\n",
    "        self.example_problem = self.problem_instance(self.example_problem_summary)\n",
    "        self.example_response = dedent(\n",
    "            f\"\"\"\n",
    "            <{self.optimizer_prompt_symbol_set.reasoning_tag}>\n",
    "            In this case, the desired response would be to change the value of input a to 14, as that would make the code return 10.\n",
    "            </{self.optimizer_prompt_symbol_set.reasoning_tag}>\n",
    "            \n",
    "            <{self.optimizer_prompt_symbol_set.improved_variable_tag}>\n",
    "            <{self.optimizer_prompt_symbol_set.name_tag}>a</{self.optimizer_prompt_symbol_set.name_tag}>\n",
    "            <{self.optimizer_prompt_symbol_set.value_tag}>\n",
    "            10\n",
    "            </{self.optimizer_prompt_symbol_set.value_tag}>\n",
    "            </{self.optimizer_prompt_symbol_set.improved_variable_tag}>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        self.include_example = include_example\n",
    "        self.max_tokens = max_tokens\n",
    "        self.log = [] if log else None\n",
    "        self.summary_log = [] if log else None\n",
    "        self.memory = FIFOBuffer(memory_size)\n",
    "\n",
    "        self.default_prompt_symbols = self.optimizer_prompt_symbol_set.default_prompt_symbols\n",
    "\n",
    "        self.prompt_symbols = copy.deepcopy(self.default_prompt_symbols)\n",
    "        self.initialize_prompt()\n",
    "\n",
    "    def initialize_prompt(self):\n",
    "        self.representation_prompt = self.representation_prompt.format(\n",
    "            variable_expression_format=dedent(f\"\"\"\n",
    "            <{self.optimizer_prompt_symbol_set.variable_tag} name=\"variable_name\" type=\"data_type\">\n",
    "            <{self.optimizer_prompt_symbol_set.value_tag}>\n",
    "            value\n",
    "            </{self.optimizer_prompt_symbol_set.value_tag}>\n",
    "            <{self.optimizer_prompt_symbol_set.constraint_tag}>\n",
    "            constraint_expression\n",
    "            </{self.optimizer_prompt_symbol_set.constraint_tag}>\n",
    "            </{self.optimizer_prompt_symbol_set.variable_tag}>\n",
    "        \"\"\"),\n",
    "            value_tag=self.optimizer_prompt_symbol_set.value_tag,\n",
    "            variables_section_title=self.optimizer_prompt_symbol_set.variables_section_title.replace(\" \", \"\"),\n",
    "            inputs_section_title=self.optimizer_prompt_symbol_set.inputs_section_title.replace(\" \", \"\"),\n",
    "            outputs_section_title=self.optimizer_prompt_symbol_set.outputs_section_title.replace(\" \", \"\"),\n",
    "            feedback_section_title=self.optimizer_prompt_symbol_set.feedback_section_title.replace(\" \", \"\"),\n",
    "            instruction_section_title=self.optimizer_prompt_symbol_set.instruction_section_title.replace(\" \", \"\"),\n",
    "            code_section_title=self.optimizer_prompt_symbol_set.code_section_title.replace(\" \", \"\"),\n",
    "            documentation_section_title=self.optimizer_prompt_symbol_set.documentation_section_title.replace(\" \", \"\"),\n",
    "            others_section_title=self.optimizer_prompt_symbol_set.others_section_title.replace(\" \", \"\")\n",
    "        )\n",
    "        self.output_format_prompt = self.output_format_prompt_template.format(\n",
    "            output_format=dedent(f\"\"\"\n",
    "            <{self.optimizer_prompt_symbol_set.reasoning_tag}>\n",
    "            reasoning\n",
    "            </{self.optimizer_prompt_symbol_set.reasoning_tag}>\n",
    "            <{self.optimizer_prompt_symbol_set.improved_variable_tag}>\n",
    "            <{self.optimizer_prompt_symbol_set.name_tag}>variable_name</{self.optimizer_prompt_symbol_set.name_tag}>\n",
    "            <{self.optimizer_prompt_symbol_set.value_tag}>\n",
    "            value\n",
    "            </{self.optimizer_prompt_symbol_set.value_tag}>\n",
    "            </{self.optimizer_prompt_symbol_set.improved_variable_tag}>\n",
    "        \"\"\"),\n",
    "            reasoning_tag=self.optimizer_prompt_symbol_set.reasoning_tag,\n",
    "            improved_variable_tag=self.optimizer_prompt_symbol_set.improved_variable_tag,\n",
    "            instruction_section_title=self.optimizer_prompt_symbol_set.instruction_section_title.replace(\" \", \"\"),\n",
    "            feedback_section_title=self.optimizer_prompt_symbol_set.feedback_section_title.replace(\" \", \"\"),\n",
    "            outputs_section_title=self.optimizer_prompt_symbol_set.outputs_section_title.replace(\" \", \"\"),\n",
    "            code_section_title=self.optimizer_prompt_symbol_set.code_section_title.replace(\" \", \"\"),\n",
    "            documentation_section_title=self.optimizer_prompt_symbol_set.documentation_section_title.replace(\" \", \"\"),\n",
    "            variables_section_title=self.optimizer_prompt_symbol_set.variables_section_title.replace(\" \", \"\"),\n",
    "            inputs_section_title=self.optimizer_prompt_symbol_set.inputs_section_title.replace(\" \", \"\"),\n",
    "            others_section_title=self.optimizer_prompt_symbol_set.others_section_title.replace(\" \", \"\")\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def repr_node_value(node_dict):\n",
    "        temp_list = []\n",
    "        for k, v in node_dict.items():\n",
    "            if \"__code\" not in k:\n",
    "                constraint_expr = f\"<constraint> ({type(v[0]).__name__}) {k}: {v[1]} </constraint>\"\n",
    "                temp_list.append(\n",
    "                    f\"<node name=\\\"{k}\\\" type=\\\"{type(v[0]).__name__}\\\">\\n<value>{v[0]}</value>\\n{constraint_expr}\\n</node>\\n\")\n",
    "            else:\n",
    "                constraint_expr = f\"<constraint>\\n{v[1]}\\n</constraint>\"\n",
    "                temp_list.append(\n",
    "                    f\"<node name=\\\"{k}\\\" type=\\\"code\\\">\\n<value>\\n{v[0]}\\n</value>\\n{constraint_expr}\\n</node>\\n\")\n",
    "        return \"\\n\".join(temp_list)\n",
    "\n",
    "    def repr_node_value_compact(self, node_dict, node_tag=\"node\",\n",
    "                                value_tag=\"value\", constraint_tag=\"constraint\"):\n",
    "        temp_list = []\n",
    "        for k, v in node_dict.items():\n",
    "            if \"__code\" not in k:\n",
    "                node_value = self.truncate_expression(v[0], self.initial_var_char_limit)\n",
    "                if v[1] is not None and node_tag == self.optimizer_prompt_symbol_set.variable_tag:\n",
    "                    constraint_expr = f\"<{constraint_tag}>\\n{v[1]}\\n</{constraint_tag}>\"\n",
    "                    temp_list.append(\n",
    "                        f\"<{node_tag} name=\\\"{k}\\\" type=\\\"{type(v[0]).__name__}\\\">\\n<{value_tag}>\\n{node_value}\\n</{value_tag}>\\n{constraint_expr}\\n</{node_tag}>\\n\")\n",
    "                else:\n",
    "                    temp_list.append(\n",
    "                        f\"<{node_tag} name=\\\"{k}\\\" type=\\\"{type(v[0]).__name__}\\\">\\n<{value_tag}>\\n{node_value}\\n</{value_tag}>\\n</{node_tag}>\\n\")\n",
    "            else:\n",
    "                constraint_expr = f\"<{constraint_tag}>\\n{v[1]}\\n</{constraint_tag}>\"\n",
    "                # we only truncate the function body\n",
    "                signature = v[1].replace(\"The code should start with:\\n\", \"\")\n",
    "                func_body = v[0].replace(signature, \"\")\n",
    "                node_value = self.truncate_expression(func_body, self.initial_var_char_limit)\n",
    "                temp_list.append(\n",
    "                    f\"<{node_tag} name=\\\"{k}\\\" type=\\\"code\\\">\\n<{value_tag}>\\n{signature}{node_value}\\n</{value_tag}>\\n{constraint_expr}\\n</{node_tag}>\\n\")\n",
    "        return \"\\n\".join(temp_list)\n",
    "\n",
    "    def truncate_expression(self, value, limit):\n",
    "        # https://stackoverflow.com/questions/1436703/what-is-the-difference-between-str-and-repr\n",
    "        value = str(value)\n",
    "        if len(value) > limit:\n",
    "            return value[:limit] + \"...(skipped due to length limit)\"\n",
    "        return value\n",
    "\n",
    "    def construct_prompt(self, summary, mask=None, *args, **kwargs):\n",
    "        \"\"\"Construct the system and user prompt.\"\"\"\n",
    "        system_prompt = (\n",
    "                self.representation_prompt + self.output_format_prompt\n",
    "        )  # generic representation + output rule\n",
    "        user_prompt = self.user_prompt_template.format(\n",
    "            problem_instance=str(self.problem_instance(summary, mask=mask))\n",
    "        )  # problem instance\n",
    "        if self.include_example:\n",
    "            user_prompt = (\n",
    "                    self.example_problem_template.format(\n",
    "                        example_problem=self.example_problem,\n",
    "                        example_response=self.example_response,\n",
    "                    )\n",
    "                    + user_prompt\n",
    "            )\n",
    "\n",
    "        var_names = []\n",
    "        for k, v in summary.variables.items():\n",
    "            var_names.append(f\"{k}\")  # ({type(v[0]).__name__})\n",
    "        var_names = \", \".join(var_names)\n",
    "\n",
    "        user_prompt += self.final_prompt.format(names=var_names)\n",
    "\n",
    "        # Add examples\n",
    "        if len(self.memory) > 0:\n",
    "            formatted_final = self.final_prompt.format(names=var_names)\n",
    "            prefix = user_prompt.split(formatted_final)[0]\n",
    "            examples = []\n",
    "            for variables, feedback in self.memory:\n",
    "                examples.append(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            \"variables\": {k: v[0] for k, v in variables.items()},\n",
    "                            \"feedback\": feedback,\n",
    "                        },\n",
    "                        indent=4,\n",
    "                    )\n",
    "                )\n",
    "            examples = \"\\n\".join(examples)\n",
    "            user_prompt = (\n",
    "                    prefix\n",
    "                    + f\"\\nBelow are some variables and their feedbacks you received in the past.\\n\\n{examples}\\n\\n\"\n",
    "                    + formatted_final\n",
    "            )\n",
    "        self.memory.add((summary.variables, summary.user_feedback))\n",
    "\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    def problem_instance(self, summary, mask=None):\n",
    "        mask = mask or []\n",
    "        return ProblemInstance(\n",
    "            instruction=self.objective if \"#Instruction\" not in mask else \"\",\n",
    "            code=(\n",
    "                \"\\n\".join([v for k, v in sorted(summary.graph)])\n",
    "                if \"#Code\" not in mask\n",
    "                else \"\"\n",
    "            ),\n",
    "            documentation=(\n",
    "                \"\\n\".join([f\"[{k}] {v}\" for k, v in summary.documentation.items()])\n",
    "                if \"#Documentation\" not in mask\n",
    "                else \"\"\n",
    "            ),\n",
    "            variables=(\n",
    "                self.repr_node_value_compact(summary.variables, node_tag=self.optimizer_prompt_symbol_set.variable_tag,\n",
    "                                             value_tag=self.optimizer_prompt_symbol_set.value_tag,\n",
    "                                             constraint_tag=self.optimizer_prompt_symbol_set.constraint_tag)\n",
    "                if \"#Variables\" not in mask\n",
    "                else \"\"\n",
    "            ),\n",
    "            inputs=(\n",
    "                self.repr_node_value_compact(summary.inputs, node_tag=self.optimizer_prompt_symbol_set.node_tag,\n",
    "                                             value_tag=self.optimizer_prompt_symbol_set.value_tag,\n",
    "                                             constraint_tag=self.optimizer_prompt_symbol_set.constraint_tag) if \"#Inputs\" not in mask else \"\"\n",
    "            ),\n",
    "            outputs=(\n",
    "                self.repr_node_value_compact(summary.output, node_tag=self.optimizer_prompt_symbol_set.node_tag,\n",
    "                                             value_tag=self.optimizer_prompt_symbol_set.value_tag,\n",
    "                                             constraint_tag=self.optimizer_prompt_symbol_set.constraint_tag) if \"#Outputs\" not in mask else \"\"\n",
    "            ),\n",
    "            others=(\n",
    "                self.repr_node_value_compact(summary.others, node_tag=self.optimizer_prompt_symbol_set.node_tag,\n",
    "                                             value_tag=self.optimizer_prompt_symbol_set.value_tag,\n",
    "                                             constraint_tag=self.optimizer_prompt_symbol_set.constraint_tag) if \"#Others\" not in mask else \"\"\n",
    "            ),\n",
    "            feedback=summary.user_feedback if \"#Feedback\" not in mask else \"\",\n",
    "            optimizer_prompt_symbol_set=self.optimizer_prompt_symbol_set\n",
    "        )\n",
    "\n",
    "    def _step(\n",
    "            self, verbose=False, mask=None, *args, **kwargs\n",
    "    ) -> Dict[ParameterNode, Any]:\n",
    "        assert isinstance(self.propagator, GraphPropagator)\n",
    "        summary = self.summarize()\n",
    "        system_prompt, user_prompt = self.construct_prompt(summary, mask=mask)\n",
    "\n",
    "        system_prompt = self.replace_symbols(system_prompt, self.prompt_symbols)\n",
    "        user_prompt = self.replace_symbols(user_prompt, self.prompt_symbols)\n",
    "\n",
    "        response = self.call_llm(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            verbose=verbose,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "\n",
    "        if \"TERMINATE\" in response:\n",
    "            return {}\n",
    "\n",
    "        suggestion = self.extract_llm_suggestion(response)\n",
    "        #print(suggestion)\n",
    "                suggestion = suggestion['variables']\n",
    "        update_dict = self.construct_update_dict(suggestion)\n",
    "        #print(update_dict)\n",
    "\n",
    "        \n",
    "        update_dict = {}\n",
    "        for node in self.parameters:\n",
    "            print(node.py_name, node.trainable)\n",
    "            formatted_suggestion = suggestion[str(node.py_name)]\n",
    "            update_dict[node] = formatted_suggestion\n",
    "\n",
    "        print(\"update_dict\", update_dict)\n",
    "        return update_dict\n",
    "\n",
    "    def extract_llm_suggestion(self, response: str):\n",
    "        \"\"\"Extract the suggestion from the response.\"\"\"\n",
    "\n",
    "        # suggestion = extract_xml_like_data(response)\n",
    "        suggestion = self.optimizer_prompt_symbol_set.output_response_extractor(response)\n",
    "\n",
    "        if len(suggestion) == 0:\n",
    "            if not self.ignore_extraction_error:\n",
    "                print(\"Cannot extract suggestion from LLM's response:\")\n",
    "                print(response)\n",
    "\n",
    "        # if the suggested value is a code, and the entire code body is empty (i.e., not even function signature is present)\n",
    "        # then we remove such suggestion\n",
    "        keys_to_remove = []\n",
    "        for key, value in suggestion.items():\n",
    "            if \"__code\" in key and value.strip() == \"\":\n",
    "                keys_to_remove.append(key)\n",
    "        for key in keys_to_remove:\n",
    "            del suggestion[key]\n",
    "\n",
    "        return suggestion\n",
    "\n",
    "    def call_llm(\n",
    "            self,\n",
    "            system_prompt: str,\n",
    "            user_prompt: str,\n",
    "            verbose: Union[bool, str] = False,\n",
    "            max_tokens: int = 4096,\n",
    "    ):\n",
    "        \"\"\"Call the LLM with a prompt and return the response.\"\"\"\n",
    "        if verbose not in (False, \"output\"):\n",
    "            print(\"Prompt\\n\", system_prompt + user_prompt)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        response = self.llm(messages=messages, max_tokens=max_tokens)\n",
    "\n",
    "        response = response.choices[0].message.content\n",
    "\n",
    "        if verbose:\n",
    "            print(\"LLM response:\\n\", response)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "695b4ac4-8861-42ad-8028-412674897587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque\n",
    "from typing import Union, List, Tuple, Dict, Any, Optional\n",
    "from opto import trace\n",
    "from opto.trainer.utils import async_run # Assuming print_color is in utils\n",
    "from opto.optimizers.utils import print_color\n",
    "from opto.trainer.algorithms.basic_algorithms import MinibatchAlgorithm, evaluate, batchify # evaluate and batchify might be useful\n",
    "import json # For LLM output parsing\n",
    "import random # Added for alpha probability\n",
    "from opto.utils.llm import LLM # For the selector LLM\n",
    "from opto.trace.nodes import ParameterNode\n",
    "import warnings\n",
    "from black import format_str, FileMode\n",
    "\n",
    "class UCBSearchAlgorithm(MinibatchAlgorithm):\n",
    "    \"\"\"\n",
    "    UCB Search Algorithm.\n",
    "\n",
    "    Keeps a buffer of candidates with their statistics (score sum, evaluation count).\n",
    "    In each iteration:\n",
    "    1. Picks a candidate 'a' from the buffer with the highest UCB score.\n",
    "    2. Updates the optimizer with 'a's parameters.\n",
    "    3. Draws a minibatch from the training set, performs a forward/backward pass, and calls optimizer.step() to get a new candidate 'a''.\n",
    "    4. Evaluates 'a'' on a validation set minibatch.\n",
    "    5. Updates statistics of 'a' (based on the training minibatch).\n",
    "    6. Adds 'a'' (with its validation stats) to the buffer.\n",
    "    7. If the buffer is full, evicts the candidate with the lowest UCB score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 agent: trace.Module,\n",
    "                 optimizer,\n",
    "                 max_buffer_size: int = 10,\n",
    "                 ucb_exploration_factor: float = 1.0,  # Controls exploration vs exploitation tradeoff in UCB selection\n",
    "                                                     # UCB formula: (a) + c * sqrt(ln(t) / n(a)), c is the exploration factor\n",
    "                 logger=None,\n",
    "                 num_threads: int = None,\n",
    "                 use_validation: bool = False,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__(agent, optimizer, num_threads=num_threads, logger=logger, *args, **kwargs)\n",
    "        \n",
    "        self.buffer = deque(maxlen=max_buffer_size) \n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        # UCB exploration factor: Higher values encourage more exploration of less-tested candidates,\n",
    "        # lower values favor exploitation of well-performing candidates. \n",
    "        self.ucb_exploration_factor = ucb_exploration_factor\n",
    "        self.use_validation = use_validation # Whether to use validation set for evaluation\n",
    "        # To ensure optimizer_step can be called with bypassing=True if needed.\n",
    "        # This depends on the specific optimizer's implementation.\n",
    "        # For now, we assume the optimizer has a step method that can return parameters.\n",
    "        if not hasattr(self.optimizer, 'step'):\n",
    "            raise ValueError(\"Optimizer must have a 'step' method.\")\n",
    "\n",
    "        self._total_evaluations_tracker = 0 # Tracks total number of individual candidate evaluations used in UCB calculation for log(T)\n",
    "        self._candidate_id_counter = 0\n",
    "\n",
    "    def _sample_minibatch(self, dataset: Dict[str, List[Any]], batch_size: int) -> Tuple[List[Any], List[Any]]:\n",
    "        \"\"\"Sample a minibatch from the dataset.\"\"\"\n",
    "        if not dataset or not dataset.get('inputs') or not dataset.get('infos'):\n",
    "            print_color(\"Warning: Attempted to sample from an empty or malformed dataset.\", color='yellow')\n",
    "            return [], []\n",
    "        \n",
    "        dataset_size = len(dataset['inputs'])\n",
    "        if dataset_size == 0:\n",
    "            print_color(\"Warning: Dataset is empty, cannot sample minibatch.\", color='yellow')\n",
    "            return [], []\n",
    "\n",
    "        actual_batch_size = min(batch_size, dataset_size)\n",
    "        indices = np.random.choice(dataset_size, actual_batch_size, replace=False)\n",
    "        xs = [dataset['inputs'][i] for i in indices]\n",
    "        infos = [dataset['infos'][i] for i in indices]\n",
    "        return xs, infos\n",
    "\n",
    "    def _evaluate_candidate(self, \n",
    "                              params_to_eval_dict: Dict[str, Any], \n",
    "                              dataset: Dict[str, List[Any]], # Changed from validate_dataset\n",
    "                              guide, # Changed from validate_guide\n",
    "                              evaluation_batch_size: int, # New parameter name\n",
    "                              num_threads: Optional[int] = None\n",
    "                              ) -> Tuple[float, int]:\n",
    "        \"\"\"Evaluates a given set of parameters on samples from the provided dataset (now typically train_dataset).\"\"\"\n",
    "        if not dataset or not dataset.get('inputs') or not dataset.get('infos') or not dataset['inputs']:\n",
    "            print_color(\"Evaluation dataset is empty or invalid. Returning score -inf, count 0.\", color='yellow')\n",
    "            return -np.inf, 0\n",
    "\n",
    "        original_params = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "        self.optimizer.update(params_to_eval_dict)      \n",
    "\n",
    "        eval_xs, eval_infos = self._sample_minibatch(dataset, evaluation_batch_size)\n",
    "        \n",
    "        if not eval_xs:\n",
    "            print_color(\"Evaluation minibatch is empty. Returning score -inf, count 0.\", color='yellow')\n",
    "            self.optimizer.update(original_params) \n",
    "            return -np.inf, 0\n",
    "\n",
    "        eval_scores = evaluate(self.agent,\n",
    "                               guide, # Use main guide\n",
    "                               eval_xs,\n",
    "                               eval_infos,\n",
    "                               min_score=self.min_score if hasattr(self, 'min_score') else None,\n",
    "                               num_threads=num_threads or self.num_threads,\n",
    "                               description=f\"Evaluating candidate\")\n",
    "\n",
    "        self.optimizer.update(original_params) \n",
    "\n",
    "        avg_score = np.mean(eval_scores) if eval_scores and all(s is not None for s in eval_scores) else 0\n",
    "        eval_count = len(eval_xs) \n",
    "        \n",
    "        return float(avg_score), eval_count\n",
    "\n",
    "    def _calculate_ucb(self, candidate_buffer_entry: Dict, total_tracked_evaluations: int) -> float:\n",
    "        \"\"\"Calculates UCB score for a candidate in the buffer.\"\"\"\n",
    "        if candidate_buffer_entry['eval_count'] == 0:\n",
    "            return float('inf')  # Explore unvisited states first\n",
    "        \n",
    "        mean_score = candidate_buffer_entry['score_sum'] / candidate_buffer_entry['eval_count']\n",
    "        \n",
    "        # Add 1 to total_tracked_evaluations to prevent log(0) if it's the first evaluation overall\n",
    "        # and to ensure log argument is > 0.\n",
    "        # Add 1 to eval_count in denominator as well to ensure it's robust if eval_count is small.\n",
    "        if total_tracked_evaluations == 0: # Should not happen if we init with one eval\n",
    "             total_tracked_evaluations = 1\n",
    "        \n",
    "        # UCB exploration term: ucb_exploration_factor scales the confidence interval\n",
    "        # Higher factor = more exploration, lower factor = more exploitation\n",
    "        exploration_term = self.ucb_exploration_factor * \\\n",
    "                           math.sqrt(math.log(total_tracked_evaluations) / candidate_buffer_entry['eval_count'])\n",
    "        \n",
    "        return mean_score + exploration_term\n",
    "    \n",
    "    def _calculate_lcb(self, candidate_buffer_entry: Dict, total_tracked_evaluations: int) -> float:\n",
    "        \"\"\"Calculates Lower Confidence Bound for a candidate in the buffer.\"\"\"\n",
    "        if candidate_buffer_entry['eval_count'] == 0:\n",
    "            return float('-inf')  # Unvisited states get lowest bound\n",
    "        \n",
    "        mean_score = candidate_buffer_entry['score_sum'] / candidate_buffer_entry['eval_count']\n",
    "        \n",
    "        # Add 1 to total_tracked_evaluations to prevent log(0) if it's the first evaluation overall\n",
    "        # and to ensure log argument is > 0.\n",
    "        # Add 1 to eval_count in denominator as well to ensure it's robust if eval_count is small.\n",
    "        if total_tracked_evaluations == 0: # Should not happen if we init with one eval\n",
    "             total_tracked_evaluations = 1\n",
    "        \n",
    "        # LCB exploration term: ucb_exploration_factor scales the confidence interval\n",
    "        # Higher factor = more exploration, lower factor = more exploitation\n",
    "        exploration_term = self.ucb_exploration_factor * \\\n",
    "                           math.sqrt(math.log(total_tracked_evaluations) / candidate_buffer_entry['eval_count'])\n",
    "        \n",
    "        return mean_score - exploration_term\n",
    "            \n",
    "    def _update_buffer_ucb_scores(self):\n",
    "        \"\"\"Recalculates and updates UCB scores for all candidates in the buffer.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "        \n",
    "        for candidate_entry in self.buffer:\n",
    "            candidate_entry['ucb_score'] = self._calculate_ucb(candidate_entry, self._total_evaluations_tracker)\n",
    "\n",
    "    def _get_best_candidate_from_buffer(self, buffer):\n",
    "        \"\"\"Get the best candidate from buffer, excluding those with eval_count = 0 when not using validation.\"\"\"\n",
    "        if not buffer:\n",
    "            return None\n",
    "        \n",
    "        # Filter out candidates with eval_count = 0 if not using validation\n",
    "        if not self.use_validation:\n",
    "            valid_candidates = [c for c in buffer if c['eval_count'] > 0]\n",
    "            if not valid_candidates:\n",
    "                # If no candidates have been evaluated, return the one with highest UCB score\n",
    "                return max(buffer, key=lambda c: c.get('ucb_score', -float('inf')))\n",
    "            return max(valid_candidates, key=lambda c: c['score_sum'] / c['eval_count'])\n",
    "        else:\n",
    "            # When using validation, all candidates should have eval_count > 0\n",
    "            return max(buffer, key=lambda c: c['score_sum'] / (c['eval_count'] or 1E-9))\n",
    "\n",
    "    def print_intervals(self, buffer):\n",
    "        \"\"\"Print confidence intervals for debugging in the form of open intervals (LCB, UCB)\"\"\"\n",
    "        print_color(\"Confidence intervals for all candidates:\", 'cyan')\n",
    "        for i, candidate_entry in enumerate(buffer):\n",
    "            lcb = self._calculate_lcb(candidate_entry, self._total_evaluations_tracker)\n",
    "            ucb = candidate_entry['ucb_score']\n",
    "            mean_score = candidate_entry['score_sum'] / (candidate_entry['eval_count'] or 1)\n",
    "            eval_count = candidate_entry['eval_count']\n",
    "            \n",
    "            # Format as open interval (LCB, UCB) with mean score and evaluation count\n",
    "            interval_str = f\"Action {i+1}: ({lcb:.4f}, {ucb:.4f}) [mean: {mean_score:.4f}, n: {eval_count}]\"\n",
    "            print_color(interval_str, 'cyan')\n",
    "\n",
    "    def _process_single_candidate(self, \n",
    "                                 action_candidate_a: Dict,\n",
    "                                 guide,\n",
    "                                 train_dataset: Dict[str, List[Any]],\n",
    "                                 validation_dataset: Dict[str, List[Any]],\n",
    "                                 train_batch_size: int,\n",
    "                                 evaluation_batch_size: int,\n",
    "                                 num_threads: Optional[int],\n",
    "                                 iteration: int) -> Tuple[bool, float, float, int]:\n",
    "        \"\"\"\n",
    "        Process a single candidate: generate a_prime, evaluate both a and a_prime,\n",
    "        update stats for 'a', and add 'a_prime' to buffer.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (success, a_prime_score, score_for_a_on_train_batch, samples_used)\n",
    "        \"\"\"\n",
    "        # 2. Load parameters of 'a' into the agent for the optimizer update step\n",
    "        self.optimizer.update(action_candidate_a['params'])\n",
    "\n",
    "        # 3. Draw minibatch from the training set, do update from 'a' to get 'a_prime'\n",
    "        train_xs, train_infos = self._sample_minibatch(train_dataset, train_batch_size)\n",
    "        if not train_xs:\n",
    "            print_color(f\"Iter {iteration}: Training minibatch empty for candidate, skipping.\", 'yellow')\n",
    "            return False, -np.inf, -np.inf, 0\n",
    "\n",
    "        # Perform forward pass and get feedback for agent parameters 'a'\n",
    "        use_asyncio = self._use_asyncio(num_threads)\n",
    "        if use_asyncio:\n",
    "            outputs_for_a = async_run([self.forward]*len(train_xs),\n",
    "                               [(self.agent, x, guide, info) for x, info in zip(train_xs, train_infos)],\n",
    "                               max_workers=num_threads,\n",
    "                               description=f\"Iter {iteration}: Forward pass for action 'a'\")\n",
    "        else:\n",
    "            outputs_for_a = [self.forward(self.agent, x, guide, info) for x, info in zip(train_xs, train_infos)]\n",
    "\n",
    "        scores_from_train, targets_from_train, feedbacks_from_train = [], [], []\n",
    "        for target, score, feedback in outputs_for_a:\n",
    "            scores_from_train.append(score)\n",
    "            targets_from_train.append(target)\n",
    "            feedbacks_from_train.append(feedback)\n",
    "        \n",
    "        if not scores_from_train:\n",
    "            print_color(f\"Iter {iteration}: No outputs from forward pass for candidate. Skipping.\", 'yellow')\n",
    "            return False, -np.inf, -np.inf, 0\n",
    "\n",
    "        target_for_a = batchify(*targets_from_train)\n",
    "        feedback_for_a = batchify(*feedbacks_from_train).data\n",
    "        score_for_a_on_train_batch = np.mean([s for s in scores_from_train if s is not None]) if any(s is not None for s in scores_from_train) else -np.inf\n",
    "\n",
    "        self.optimizer.zero_feedback()\n",
    "        self.optimizer.backward(target_for_a, feedback_for_a)\n",
    "\n",
    "        try:\n",
    "            a_prime_params_dict = self.optimizer.step(bypassing=True, verbose=False) \n",
    "            if not isinstance(a_prime_params_dict, dict) or not a_prime_params_dict:\n",
    "                print_color(f\"Iter {iteration}: Optimizer.step did not return valid params. Using current agent params.\", 'yellow')\n",
    "                a_prime_params_dict = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "            self.total_proposals += 1\n",
    "        except Exception as e:\n",
    "            print_color(f\"Iter {iteration}: Error during optimizer.step: {e}. Skipping.\", 'red')\n",
    "            return False, -np.inf, -np.inf, 0\n",
    "        \n",
    "        # 4. Evaluate 'a' and 'a_prime' on samples of validation set in parallel\n",
    "        if self.use_validation:\n",
    "            if use_asyncio:\n",
    "                evaluation_results = async_run(\n",
    "                    [self._evaluate_candidate, self._evaluate_candidate],\n",
    "                    [\n",
    "                        (action_candidate_a['params'], validation_dataset, guide, evaluation_batch_size, num_threads),\n",
    "                        (a_prime_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads)\n",
    "                    ],\n",
    "                    max_workers=2,\n",
    "                    description=f\"Iter {iteration}: Parallel evaluation of 'a' and 'a_prime'\"\n",
    "                )\n",
    "                (a_score, a_evals), (a_prime_score, a_prime_evals) = evaluation_results\n",
    "            else:\n",
    "                a_score, a_evals = self._evaluate_candidate(\n",
    "                    action_candidate_a['params'], validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "                )\n",
    "                a_prime_score, a_prime_evals = self._evaluate_candidate(\n",
    "                    a_prime_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "                )\n",
    "        \n",
    "        # 5. Update statistics for the original candidate 'a' \n",
    "        # Always update statistics for the original candidate 'a' on the training set\n",
    "        if score_for_a_on_train_batch > -np.inf:\n",
    "            action_candidate_a['score_sum'] += score_for_a_on_train_batch * len(train_xs)\n",
    "            action_candidate_a['eval_count'] += len(train_xs)\n",
    "            self._total_evaluations_tracker += len(train_xs)\n",
    "\n",
    "        # If we use validation set for evaluation\n",
    "        if self.use_validation: # If we use validation set for evaluation\n",
    "            action_candidate_a['score_sum'] += a_score * a_evals\n",
    "            action_candidate_a['eval_count'] += a_evals\n",
    "        \n",
    "        # 6. Add 'a_prime' to the buffer (with eviction logic if needed)\n",
    "            if a_prime_score > -np.inf and a_prime_evals > 0:\n",
    "                new_candidate_entry = {\n",
    "                    'params': a_prime_params_dict,\n",
    "                    'score_sum': a_prime_score * a_prime_evals,\n",
    "                    'eval_count': a_prime_evals,\n",
    "                    'ucb_score': None,  # Will be updated later\n",
    "                    'iteration_created': iteration\n",
    "                }\n",
    "                \n",
    "                # Eviction logic before adding if buffer is at max capacity\n",
    "                if len(self.buffer) >= self.max_buffer_size:\n",
    "                    self._update_buffer_ucb_scores()  # Ensure UCBs are current before eviction\n",
    "                    candidate_to_evict = min(self.buffer, key=lambda c: c['ucb_score'])\n",
    "                    self.buffer.remove(candidate_to_evict)\n",
    "                    print_color(f\"Iter {iteration}: Buffer full. Evicted candidate (UCB: {candidate_to_evict['ucb_score']:.4f})\", 'magenta')\n",
    "                \n",
    "                self.buffer.append(new_candidate_entry)\n",
    "                print_color(f\"Iter {iteration}: Added new candidate to buffer (score: {a_prime_score:.4f})\", 'magenta')\n",
    "            else:\n",
    "                print_color(f\"Iter {iteration}: New candidate a_prime had invalid score/evals, not added to buffer.\", 'yellow')\n",
    "\n",
    "            # Update tracking\n",
    "            self._total_evaluations_tracker += a_evals + a_prime_evals\n",
    "            samples_used = 2 * evaluation_batch_size + train_batch_size\n",
    "        else: # If we don't use validation set for evaluation, please evaluate a_prime on the training set\n",
    "            a_prime_score, a_prime_evals = self._evaluate_candidate(\n",
    "                a_prime_params_dict, {'inputs': train_xs, 'infos': train_infos}, \n",
    "                guide, len(train_xs), num_threads\n",
    "            )\n",
    "            self._total_evaluations_tracker += a_prime_evals\n",
    "            \n",
    "            new_candidate_entry = {\n",
    "                    'params': a_prime_params_dict,\n",
    "                    'score_sum': a_prime_score * a_prime_evals if a_prime_score > -np.inf else 0,\n",
    "                    'eval_count': a_prime_evals,\n",
    "                    'ucb_score': None,  # Will be updated later\n",
    "                    'iteration_created': iteration\n",
    "                }\n",
    "            self.buffer.append(new_candidate_entry)\n",
    "            samples_used = 2*train_batch_size  # One batch for training update, one for evaluation\n",
    "        return True, a_prime_score, score_for_a_on_train_batch, samples_used\n",
    "\n",
    "    def train(self,\n",
    "              guide,  # Guide for train_dataset (feedback generation AND evaluation)\n",
    "              train_dataset: Dict[str, List[Any]],\n",
    "              *,\n",
    "              validation_dataset: Optional[Dict[str, List[Any]]] = None,  # Validation set for evaluation, defaults to train_dataset\n",
    "              test_dataset: Optional[Dict[str, List[Any]]] = None,\n",
    "              num_search_iterations: int = 100,\n",
    "              train_batch_size: int = 2, \n",
    "              evaluation_batch_size: int = 20, # Renamed from validation_batch_size, used for all explicit evaluations\n",
    "              eval_frequency: int = 1, \n",
    "              log_frequency: Optional[int] = None,\n",
    "              save_frequency: Optional[int] = None,\n",
    "              save_path: str = \"checkpoints/ucb_agent.pkl\",\n",
    "              min_score_for_agent_update: Optional[float] = None, # Renamed from min_score to avoid conflict with evaluate's min_score\n",
    "              verbose: Union[bool, str] = False,\n",
    "              num_threads: Optional[int] = None,\n",
    "              print_confidence_interval: bool = True,\n",
    "              **kwargs\n",
    "              ) -> Tuple[Dict[str, Any], float]: # Returns metrics and best score\n",
    "        \"\"\"\n",
    "        Main training loop for UCB Search Algorithm.\n",
    "        \"\"\"\n",
    "        # Default validation_dataset to train_dataset if not provided\n",
    "        if validation_dataset is None:\n",
    "            validation_dataset = train_dataset\n",
    "        if test_dataset is None:\n",
    "            test_dataset = train_dataset\n",
    "\n",
    "        num_threads = num_threads or self.num_threads\n",
    "        log_frequency = log_frequency or eval_frequency\n",
    "        self.min_score = min_score_for_agent_update # Used by parent's evaluate if called, or our own _evaluate_candidate\n",
    "        total_samples = 0\n",
    "        self.total_proposals = 0\n",
    "        # Metrics tracking\n",
    "        metrics = {\n",
    "            'best_candidate_scores': [], # Score of the best candidate (e.g., highest mean) found so far at each iteration\n",
    "            'selected_action_ucb': [], # UCB score of the selected action 'a'\n",
    "            'new_candidate_scores': [], # Score of the new candidate 'a_prime'\n",
    "            'buffer_avg_score': [],\n",
    "            'buffer_avg_evals': [],\n",
    "        }\n",
    "\n",
    "# 0. Evaluate the initial parameter on samples of the validation set and add it to the buffer.\n",
    "        initial_params_dict = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "        print_color(\"Evaluating initial parameters using validation_dataset samples...\", 'cyan')\n",
    "        initial_score, initial_evals = self._evaluate_candidate(\n",
    "            initial_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads # Use validation_dataset and guide\n",
    "        )\n",
    "        self.logger.log('Test score', initial_score, 0, color='blue')\n",
    "        self.logger.log('Total samples', total_samples, 0, color='cyan')\n",
    "        print_color(f\"Initial candidate: Score {initial_score:.4f}, Evals {initial_evals}\", 'yellow')\n",
    "        if self.use_validation:\n",
    "            self._total_evaluations_tracker += initial_evals \n",
    "            total_samples += initial_evals\n",
    "            # Log initial evaluation\n",
    "            initial_candidate_entry = {\n",
    "                'params': initial_params_dict,\n",
    "                'score_sum': initial_score * initial_evals if initial_score > -np.inf else 0, # Store sum for accurate mean later\n",
    "                'eval_count': initial_evals,\n",
    "                'ucb_score': None, # avoid accidental reads before it's initialized\n",
    "                'iteration_created': 0\n",
    "            }\n",
    "            self._update_buffer_ucb_scores() # Update UCB for the initial candidate\n",
    "        else:\n",
    "            initial_candidate_entry = {\n",
    "                'params': initial_params_dict,\n",
    "                'score_sum': 0,\n",
    "                'eval_count': 0,\n",
    "                'ucb_score': None, # avoid accidental reads before it's initialized\n",
    "                'iteration_created': 0\n",
    "            }\n",
    "        self.buffer.append(initial_candidate_entry)\n",
    "\n",
    "        # Main search loop\n",
    "        for iteration in range(1, num_search_iterations + 1):\n",
    "            try:\n",
    "                if not self.buffer:\n",
    "                    print_color(\"Buffer is empty, stopping search.\", 'red')\n",
    "                    break\n",
    "\n",
    "                # 1. Pick the candidate 'a' with the highest UCB from the buffer\n",
    "                self._update_buffer_ucb_scores() # Ensure UCB scores are fresh\n",
    "                    \n",
    "                action_candidate_a = self.select(self.buffer)\n",
    "                if print_confidence_interval:\n",
    "                    self.print_intervals(self.buffer)\n",
    "                # Log selected action UCB score\n",
    "                self.logger.log('Selected action UCB', action_candidate_a['ucb_score'], iteration, color='magenta')\n",
    "                self.logger.log('Selected action mean score', action_candidate_a['score_sum']/(action_candidate_a['eval_count'] or 1), iteration, color='cyan')\n",
    "                \n",
    "                print_color(f\"Iter {iteration}/{num_search_iterations}: \", 'blue')\n",
    "                \n",
    "                # Process the selected candidate\n",
    "                success, a_prime_score, score_for_a_on_train_batch, samples_used = self._process_single_candidate(\n",
    "                    action_candidate_a, guide, train_dataset, validation_dataset,\n",
    "                    train_batch_size, evaluation_batch_size, num_threads, iteration\n",
    "                )\n",
    "                \n",
    "                if not success:  # Error occurred in processing\n",
    "                    continue\n",
    "                    \n",
    "                total_samples += samples_used\n",
    "                if self.use_validation:\n",
    "                    metrics['new_candidate_scores'].append(a_prime_score)\n",
    "                    self.logger.log('New candidate score', a_prime_score, iteration, color='green')\n",
    "                    print_color(f\"Iter {iteration}: New candidate a_prime generated. Validation Score: {a_prime_score:.4f}\", 'cyan')\n",
    "                self.logger.log('Training batch score', score_for_a_on_train_batch, iteration, color='yellow')\n",
    "                \n",
    "                \n",
    "\n",
    "                # Update all UCB scores in the buffer after potential additions/removals/stat updates\n",
    "                self._update_buffer_ucb_scores()\n",
    "\n",
    "                # Logging\n",
    "                best_in_buffer = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                if best_in_buffer:\n",
    "                    metrics['best_candidate_scores'].append(best_in_buffer['score_sum']/(best_in_buffer['eval_count'] or 1))\n",
    "                else:\n",
    "                    metrics['best_candidate_scores'].append(-np.inf)\n",
    "                metrics['buffer_avg_score'].append(np.mean([c['score_sum']/(c['eval_count'] or 1) for c in self.buffer if c['eval_count'] > 0]))\n",
    "                metrics['buffer_avg_evals'].append(np.mean([c['eval_count'] for c in self.buffer]))\n",
    "\n",
    "                if iteration % log_frequency == 0:\n",
    "                    log_data = {\n",
    "                        \"iteration\": iteration,\n",
    "                        \"best_score\": metrics['best_candidate_scores'][-1], #best_candidate_score_in_buffer\n",
    "                        \"selected_action_ucb\": action_candidate_a['ucb_score'],\n",
    "                        \"new_candidate_score\": a_prime_score,\n",
    "                        \"buffer_size\": len(self.buffer),\n",
    "                        \"buffer_avg_score\": metrics['buffer_avg_score'][-1],\n",
    "                        \"buffer_avg_evals\": metrics['buffer_avg_evals'][-1],\n",
    "                        \"total_evaluations_tracker\": self._total_evaluations_tracker, # used in calculating ucb scores\n",
    "                        \"total_samples\": total_samples # Add new metric\n",
    "                    }\n",
    "                    \n",
    "                    # Log all important metrics\n",
    "                    self.logger.log('Best candidate score', log_data['best_score'], iteration, color='green')\n",
    "                    self.logger.log('Buffer size', log_data['buffer_size'], iteration, color='blue')\n",
    "                    self.logger.log('Buffer average score', log_data['buffer_avg_score'], iteration, color='cyan')\n",
    "                    self.logger.log('Buffer average evaluations', log_data['buffer_avg_evals'], iteration, color='orange')\n",
    "                    # self.logger.log('Total evaluations tracker', log_data['total_evaluations_tracker'], iteration, color='magenta')\n",
    "                    self.logger.log('Total samples', log_data['total_samples'], iteration, color='yellow')\n",
    "                    self.logger.log('Total proposals', self.total_proposals, iteration, color='red')\n",
    "                    print_color(f\"Log @ Iter {iteration}: Best score in buffer: {log_data['best_score']:.4f}, Buffer size: {log_data['buffer_size']}, Total samples: {total_samples}\", 'green')\n",
    "\n",
    "                if test_dataset is not None and iteration % eval_frequency == 0:\n",
    "                    try:\n",
    "                        # Save current agent parameters\n",
    "                        current_params = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "                        \n",
    "                        # Find the best candidate in the buffer (highest mean score)\n",
    "                        best_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                        if not best_candidate:\n",
    "                            print_color(f\"Iter {iteration}: No valid candidate for test evaluation.\", 'yellow')\n",
    "                            continue\n",
    "                        \n",
    "                        # Load best candidate's parameters into the agent for evaluation\n",
    "                        self.optimizer.update(best_candidate['params'])\n",
    "                        \n",
    "                        # Evaluate the best candidate on test set\n",
    "                        test_score = self.evaluate(self.agent, guide, test_dataset['inputs'], test_dataset['infos'],\n",
    "                                      min_score=self.min_score, num_threads=num_threads,\n",
    "                                      description=f\"Evaluating best candidate (iteration {iteration})\")\n",
    "                        \n",
    "                        # Restore original agent parameters\n",
    "                        self.optimizer.update(current_params)\n",
    "                        \n",
    "                        self.logger.log('Test score', test_score, iteration, color='green')\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration}: Test evaluation failed: {e}\", 'red')\n",
    "                    \n",
    "                # Save agent (e.g., the one with highest mean score in buffer)\n",
    "                if save_frequency is not None and iteration % save_frequency == 0:\n",
    "                    try:\n",
    "                        best_overall_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                        if not best_overall_candidate:\n",
    "                            print_color(f\"Iter {iteration}: No valid candidate for agent save.\", 'yellow')\n",
    "                            continue\n",
    "                        self.optimizer.update(best_overall_candidate['params']) # Load params using optimizer\n",
    "                        self.save_agent(save_path, iteration) # save_agent is from AlgorithmBase\n",
    "                        print_color(f\"Iter {iteration}: Saved agent based on best candidate in buffer.\", 'green')\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration}: Agent save failed: {e}\", 'red')\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print_color(f\"Iter {iteration}: Iteration failed with error: {e}. Skipping to next iteration.\", 'red')\n",
    "                self.logger.log('Iteration error', str(e), iteration, color='red')\n",
    "                continue\n",
    "\n",
    "        # End of search loop\n",
    "        print_color(\"UCB search finished.\", 'blue')\n",
    "        \n",
    "        # Log final training summary\n",
    "        final_iteration = num_search_iterations\n",
    "        self.logger.log('UCB search completed', final_iteration, final_iteration, color='blue')\n",
    "        self.logger.log('Final total samples', total_samples, final_iteration, color='magenta')\n",
    "        \n",
    "        if not self.buffer:\n",
    "            print_color(\"Buffer is empty at the end of search. No best candidate found.\", 'red')\n",
    "            self.logger.log('Final status', 'Buffer empty - no best candidate', final_iteration, color='red')\n",
    "            return metrics, -np.inf\n",
    "            \n",
    "        # Select the best candidate based on highest mean score (exploitation)\n",
    "        final_best_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "        if not final_best_candidate:\n",
    "            print_color(\"No valid candidate found at the end of search.\", 'red')\n",
    "            return metrics, -np.inf\n",
    "        final_best_score = final_best_candidate['score_sum'] / (final_best_candidate['eval_count'] or 1E-9)\n",
    "        \n",
    "        # Log final results\n",
    "        self.logger.log('Final best score', final_best_score, final_iteration, color='green')\n",
    "        self.logger.log('Final best candidate evaluations', final_best_candidate['eval_count'], final_iteration, color='cyan')\n",
    "        self.logger.log('Final buffer size', len(self.buffer), final_iteration, color='blue')\n",
    "        \n",
    "        print_color(f\"Final best candidate: Mean Score {final_best_score:.4f}, Evals {final_best_candidate['eval_count']}\", 'green')\n",
    "\n",
    "        # Load best parameters into the agent\n",
    "        self.optimizer.update(final_best_candidate['params']) # Load params using optimizer\n",
    "\n",
    "        return metrics, float(final_best_score)\n",
    "    \n",
    "    def select(self, buffer):\n",
    "        '''Could be subclassed to implement different selection strategies'''\n",
    "        return max(buffer, key=lambda c: c['ucb_score'])\n",
    "\n",
    "\n",
    "class UCBSearchParallelAlgorithm(UCBSearchAlgorithm):\n",
    "    \"\"\"\n",
    "    Parallel UCB Search Algorithm.\n",
    "    \n",
    "    Instead of selecting one candidate with highest UCB score, selects top-k candidates\n",
    "    and processes them in parallel to generate k new candidates per iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 agent: trace.Module,\n",
    "                 optimizer,\n",
    "                 max_buffer_size: int = 10,\n",
    "                 ucb_exploration_factor: float = 1.0,\n",
    "                 parallel_k: int = 2,  # Number of top candidates to process in parallel\n",
    "                 logger=None,\n",
    "                 num_threads: int = None,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__(agent, optimizer, max_buffer_size, ucb_exploration_factor, \n",
    "                         logger, num_threads, *args, **kwargs)\n",
    "        self.parallel_k = parallel_k\n",
    "\n",
    "    def select_top_k(self, buffer, k):\n",
    "        \"\"\"Select top k candidates with highest UCB scores\"\"\"\n",
    "        if len(buffer) <= k:\n",
    "            return buffer.copy()\n",
    "        \n",
    "        # Sort by UCB score and return top k\n",
    "        sorted_candidates = sorted(buffer, key=lambda c: c['ucb_score'], reverse=True)\n",
    "        return sorted_candidates[:k]\n",
    "\n",
    "    def train(self,\n",
    "              guide,\n",
    "              train_dataset: Dict[str, List[Any]],\n",
    "              *,\n",
    "              validation_dataset: Optional[Dict[str, List[Any]]] = None,\n",
    "              test_dataset: Optional[Dict[str, List[Any]]] = None,\n",
    "              num_search_iterations: int = 100,\n",
    "              train_batch_size: int = 2,\n",
    "              evaluation_batch_size: int = 20,\n",
    "              eval_frequency: int = 1,\n",
    "              log_frequency: Optional[int] = None,\n",
    "              save_frequency: Optional[int] = None,\n",
    "              save_path: str = \"checkpoints/ucb_parallel_agent.pkl\",\n",
    "              min_score_for_agent_update: Optional[float] = None,\n",
    "              verbose: Union[bool, str] = False,\n",
    "              num_threads: Optional[int] = None,\n",
    "              print_confidence_interval: bool = True,\n",
    "              **kwargs\n",
    "              ) -> Tuple[Dict[str, Any], float]:\n",
    "        \"\"\"\n",
    "        Main training loop for Parallel UCB Search Algorithm.\n",
    "        \"\"\"\n",
    "        # Default validation_dataset to train_dataset if not provided\n",
    "        if validation_dataset is None:\n",
    "            validation_dataset = train_dataset\n",
    "        if test_dataset is None:\n",
    "            test_dataset = train_dataset\n",
    "\n",
    "        num_threads = num_threads or self.num_threads\n",
    "        log_frequency = log_frequency or eval_frequency\n",
    "        self.min_score = min_score_for_agent_update\n",
    "        total_samples = 0\n",
    "        self.total_proposals = 0\n",
    "        \n",
    "        # Metrics tracking\n",
    "        metrics = {\n",
    "            'best_candidate_scores': [],\n",
    "            'selected_actions_ucb': [],  # UCB scores of selected top-k actions\n",
    "            'new_candidate_scores': [],  # Scores of all new candidates\n",
    "            'buffer_avg_score': [],\n",
    "            'buffer_avg_evals': [],\n",
    "            'parallel_k_used': [],  # Track how many candidates were actually processed\n",
    "        }\n",
    "\n",
    "        # Initialize with first candidate (same as parent)\n",
    "        print_color(\"Evaluating initial parameters using validation_dataset samples...\", 'cyan')\n",
    "        initial_params_dict = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "        initial_score, initial_evals = self._evaluate_candidate(\n",
    "            initial_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "        )\n",
    "        self._total_evaluations_tracker += initial_evals \n",
    "        total_samples += initial_evals\n",
    "\n",
    "        # Log initial evaluation\n",
    "        self.logger.log('Initial UCB score', initial_score, 0, color='blue')\n",
    "        self.logger.log('Total samples', total_samples, 0, color='cyan')\n",
    "\n",
    "        initial_candidate_entry = {\n",
    "            'params': initial_params_dict,\n",
    "            'score_sum': initial_score * initial_evals if initial_score > -np.inf else 0,\n",
    "            'eval_count': initial_evals,\n",
    "            'ucb_score': None,\n",
    "            'iteration_created': 0\n",
    "        }\n",
    "        self.buffer.append(initial_candidate_entry)\n",
    "        self._update_buffer_ucb_scores()\n",
    "        print_color(f\"Initial candidate: Score {initial_score:.4f}, Evals {initial_evals}\", 'yellow')\n",
    "\n",
    "        # Main search loop\n",
    "        for iteration in range(1, num_search_iterations + 1):\n",
    "            try:\n",
    "                if not self.buffer:\n",
    "                    print_color(\"Buffer is empty, stopping search.\", 'red')\n",
    "                    break\n",
    "\n",
    "                # 1. Select top-k candidates with highest UCB scores\n",
    "                self._update_buffer_ucb_scores()\n",
    "                top_k_candidates = self.select_top_k(self.buffer, self.parallel_k)\n",
    "                \n",
    "                if print_confidence_interval:\n",
    "                    self.print_intervals(self.buffer)\n",
    "                \n",
    "                print_color(f\"Iter {iteration}/{num_search_iterations}: Processing {len(top_k_candidates)} candidates in parallel\", 'blue')\n",
    "                \n",
    "                # Log selected actions UCB scores\n",
    "                selected_ucb_scores = [c['ucb_score'] for c in top_k_candidates]\n",
    "                metrics['selected_actions_ucb'].append(selected_ucb_scores)\n",
    "                avg_selected_ucb = np.mean(selected_ucb_scores)\n",
    "                self.logger.log('Average selected UCB', avg_selected_ucb, iteration, color='magenta')\n",
    "\n",
    "                # 2. Process all top-k candidates sequentially\n",
    "                candidate_results = []\n",
    "                for candidate in top_k_candidates:\n",
    "                    result = self._process_single_candidate(\n",
    "                        candidate, guide, train_dataset, validation_dataset,\n",
    "                        train_batch_size, evaluation_batch_size, num_threads, iteration\n",
    "                    )\n",
    "                    candidate_results.append(result)\n",
    "\n",
    "                # 3. Process results and update statistics\n",
    "                iteration_new_scores = []\n",
    "                \n",
    "                for i, (candidate, result) in enumerate(zip(top_k_candidates, candidate_results)):\n",
    "                    success, a_prime_score, score_for_a_on_train_batch, samples_used = result\n",
    "                    \n",
    "                    if not success:  # Error occurred\n",
    "                        print_color(f\"Iter {iteration}: Candidate {i+1} processing failed, skipping.\", 'yellow')\n",
    "                        continue                \n",
    "                    # Track new candidate score\n",
    "                    iteration_new_scores.append(a_prime_score)\n",
    "                    \n",
    "                    # Update tracking\n",
    "                    total_samples += samples_used\n",
    "\n",
    "                metrics['new_candidate_scores'].extend(iteration_new_scores)\n",
    "                \n",
    "                # Log iteration performance\n",
    "                if iteration_new_scores:\n",
    "                    avg_new_score = np.mean(iteration_new_scores)\n",
    "                    max_new_score = max(iteration_new_scores)\n",
    "                    self.logger.log('New candidate score', avg_new_score, iteration, color='green') #average new candidate score\n",
    "                    self.logger.log('Max new candidate score', max_new_score, iteration, color='green')\n",
    "                    print_color(f\"Iter {iteration}: Generated {len(iteration_new_scores)} new candidates. Avg score: {avg_new_score:.4f}, Max: {max_new_score:.4f}\", 'cyan')\n",
    "\n",
    "                # Update UCB scores and track metrics\n",
    "                self._update_buffer_ucb_scores()\n",
    "                \n",
    "                if self.buffer:\n",
    "                    best_in_buffer = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                    if best_in_buffer:\n",
    "                        best_score = best_in_buffer['score_sum']/(best_in_buffer['eval_count'] or 1)\n",
    "                        metrics['best_candidate_scores'].append(best_score)\n",
    "                    else:\n",
    "                        metrics['best_candidate_scores'].append(-np.inf)\n",
    "                    metrics['buffer_avg_score'].append(np.mean([c['score_sum']/(c['eval_count'] or 1) for c in self.buffer if c['eval_count'] > 0]))\n",
    "                    metrics['buffer_avg_evals'].append(np.mean([c['eval_count'] for c in self.buffer]))\n",
    "\n",
    "                    # Logging\n",
    "                    if iteration % log_frequency == 0:\n",
    "                        self.logger.log('Best candidate score', best_score, iteration, color='green')\n",
    "                        self.logger.log('Buffer size', len(self.buffer), iteration, color='blue')\n",
    "                        self.logger.log('Buffer average score', metrics['buffer_avg_score'][-1], iteration, color='cyan')\n",
    "                        self.logger.log('Total samples', total_samples, iteration, color='yellow')\n",
    "                        self.logger.log('Total proposals', self.total_proposals, iteration, color='red')\n",
    "                        print_color(f\"Log @ Iter {iteration}: Best score: {best_score:.4f}, Buffer size: {len(self.buffer)}, Total samples: {total_samples}\", 'green')\n",
    "\n",
    "                # Test evaluation (same as parent)\n",
    "                if test_dataset is not None and iteration % eval_frequency == 0:\n",
    "                    try:\n",
    "                        current_params = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "                        best_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                        if not best_candidate:\n",
    "                            print_color(f\"Iter {iteration}: No valid candidate for test evaluation.\", 'yellow')\n",
    "                            continue\n",
    "                        self.optimizer.update(best_candidate['params'])\n",
    "                        \n",
    "                        test_score = self.evaluate(self.agent, guide, test_dataset['inputs'], test_dataset['infos'],\n",
    "                                      min_score=self.min_score, num_threads=num_threads,\n",
    "                                      description=f\"Evaluating best candidate (iteration {iteration})\")\n",
    "                        \n",
    "                        self.optimizer.update(current_params)\n",
    "                        self.logger.log('Test score', test_score, iteration, color='green')\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration}: Test evaluation failed: {e}\", 'red')\n",
    "                    \n",
    "                # Save agent (same as parent)\n",
    "                if save_frequency is not None and iteration % save_frequency == 0:\n",
    "                    try:\n",
    "                        best_overall_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                        if not best_overall_candidate:\n",
    "                            print_color(f\"Iter {iteration}: No valid candidate for agent save.\", 'yellow')\n",
    "                            continue\n",
    "                        self.optimizer.update(best_overall_candidate['params'])\n",
    "                        self.save_agent(save_path, iteration)\n",
    "                        print_color(f\"Iter {iteration}: Saved agent based on best candidate in buffer.\", 'green')\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration}: Agent save failed: {e}\", 'red')\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print_color(f\"Iter {iteration}: Iteration failed with error: {e}. Skipping to next iteration.\", 'red')\n",
    "                self.logger.log('Iteration error', str(e), iteration, color='red')\n",
    "                continue\n",
    "\n",
    "        # End of search (same as parent)\n",
    "        print_color(\"Parallel UCB search finished.\", 'blue')\n",
    "        \n",
    "        final_iteration = num_search_iterations\n",
    "        self.logger.log('Parallel UCB search completed', final_iteration, final_iteration, color='blue')\n",
    "        self.logger.log('Final total samples', total_samples, final_iteration, color='magenta')\n",
    "        \n",
    "        if not self.buffer:\n",
    "            print_color(\"Buffer is empty at the end of search. No best candidate found.\", 'red')\n",
    "            return metrics, -np.inf\n",
    "            \n",
    "        final_best_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "        if not final_best_candidate:\n",
    "            print_color(\"No valid candidate found at the end of search.\", 'red')\n",
    "            return metrics, -np.inf\n",
    "        final_best_score = final_best_candidate['score_sum'] / (final_best_candidate['eval_count'] or 1E-9)\n",
    "        \n",
    "        self.logger.log('Final best score', final_best_score, final_iteration, color='green')\n",
    "        print_color(f\"Final best candidate: Mean Score {final_best_score:.4f}, Evals {final_best_candidate['eval_count']}\", 'green')\n",
    "\n",
    "        # Load best parameters into the agent\n",
    "        self.optimizer.update(final_best_candidate['params'])\n",
    "\n",
    "        return metrics, float(final_best_score)\n",
    "    \n",
    "\n",
    "class HybridUCB_LLM(MinibatchAlgorithm):\n",
    "    \"\"\"\n",
    "    UCB Search Algorithm with Function Approximation (LLM).\n",
    "\n",
    "    Keeps a buffer of candidates.\n",
    "    In each iteration:\n",
    "    - With probability alpha:\n",
    "        1. Picks a candidate 'a' from the buffer with the highest UCB score.\n",
    "        2. Updates the optimizer with 'a's parameters.\n",
    "        3. Draws a minibatch from the training set, performs a forward/backward pass, and calls optimizer.step() to get a new candidate 'a_prime'.\n",
    "        4. Evaluates 'a_prime' on a validation set minibatch.\n",
    "        5. Updates statistics of 'a' (based on the training minibatch).\n",
    "        6. Adds 'a_prime' (with its validation stats) to the buffer.\n",
    "    - With probability 1-alpha:\n",
    "        1. Uses an external LLM, prompted with candidates from the buffer, to generate a new candidate 'a_prime'.\n",
    "        2. Evaluates 'a_prime' on a validation set minibatch.\n",
    "        3. Adds 'a_prime' (with its validation stats) to the buffer.\n",
    "    If the buffer is full, evicts the candidate with the lowest UCB score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 agent: trace.Module,\n",
    "                 optimizer,\n",
    "                 max_buffer_size: int = 10,\n",
    "                 ucb_exploration_factor: float = 0.3,\n",
    "                 alpha: float = 0.3,\n",
    "                 llm_model: str = None,\n",
    "                 num_samples_in_prompt: int = 5,\n",
    "                 logger=None,\n",
    "                 num_threads: int = None,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__(agent, optimizer, num_threads=num_threads, logger=logger, *args, **kwargs)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.llm_model = llm_model\n",
    "        self.num_samples_in_prompt = num_samples_in_prompt\n",
    "        self.llm_prompt_budget_factor = 0.5\n",
    "        \n",
    "        self.buffer = deque(maxlen=max_buffer_size) \n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        self.ucb_exploration_factor = ucb_exploration_factor\n",
    "\n",
    "        if not hasattr(self.optimizer, 'step'):\n",
    "            raise ValueError(\"Optimizer must have a 'step' method.\")\n",
    "\n",
    "        self._total_evaluations_tracker = 0\n",
    "\n",
    "        # Initialize LLM\n",
    "        self.llm = LLM(model=self.llm_model)\n",
    "        print_color(f\"Initialized HybridUCB_LLM with alpha={self.alpha}, LLM model={self.llm_model}\", \"cyan\")\n",
    "\n",
    "    def _sample_minibatch(self, dataset: Dict[str, List[Any]], batch_size: int) -> Tuple[List[Any], List[Any]]:\n",
    "        \"\"\"Sample a minibatch from the dataset.\"\"\"\n",
    "        if not dataset or not dataset.get('inputs') or not dataset.get('infos'):\n",
    "            print_color(\"Warning: Attempted to sample from an empty or malformed dataset.\", color='yellow')\n",
    "            return [], []\n",
    "        \n",
    "        dataset_size = len(dataset['inputs'])\n",
    "        if dataset_size == 0:\n",
    "            print_color(\"Warning: Dataset is empty, cannot sample minibatch.\", color='yellow')\n",
    "            return [], []\n",
    "\n",
    "        actual_batch_size = min(batch_size, dataset_size)\n",
    "        indices = np.random.choice(dataset_size, actual_batch_size, replace=False)\n",
    "        xs = [dataset['inputs'][i] for i in indices]\n",
    "        infos = [dataset['infos'][i] for i in indices]\n",
    "        return xs, infos\n",
    "\n",
    "    def _evaluate_candidate(self, \n",
    "                              params_to_eval_dict: Dict[str, Any], \n",
    "                              dataset: Dict[str, List[Any]], \n",
    "                              guide, \n",
    "                              evaluation_batch_size: int, \n",
    "                              num_threads: Optional[int] = None\n",
    "                              ) -> Tuple[float, int]:\n",
    "        \"\"\"Evaluates a given set of parameters on samples from the provided dataset.\"\"\"\n",
    "        if not dataset or not dataset.get('inputs') or not dataset.get('infos') or not dataset['inputs']:\n",
    "            print_color(\"Evaluation dataset is empty or invalid. Returning score -inf, count 0.\", color='yellow')\n",
    "            return -np.inf, 0\n",
    "\n",
    "        original_params_backup = {p: copy.deepcopy(p.data) for p in self.agent.parameters()}\n",
    "        \n",
    "        try:\n",
    "            self.optimizer.update(params_to_eval_dict)\n",
    "        except Exception as e:\n",
    "            print_color(f\"Error updating agent with params_to_eval_dict: {e}. Using current agent state for eval.\", \"red\")\n",
    "\n",
    "        eval_xs, eval_infos = self._sample_minibatch(dataset, evaluation_batch_size)\n",
    "        \n",
    "        if not eval_xs:\n",
    "            print_color(\"Evaluation minibatch is empty. Returning score -inf, count 0.\", color='yellow')\n",
    "            self.optimizer.update(original_params_backup)\n",
    "            return -np.inf, 0\n",
    "\n",
    "        eval_scores = evaluate(self.agent,\n",
    "                               guide,\n",
    "                               eval_xs,\n",
    "                               eval_infos,\n",
    "                               min_score=self.min_score if hasattr(self, 'min_score') else None,\n",
    "                               num_threads=num_threads or self.num_threads,\n",
    "                               description=f\"Evaluating candidate\")\n",
    "\n",
    "        self.optimizer.update(original_params_backup)\n",
    "\n",
    "        avg_score = np.mean(eval_scores) if eval_scores and all(s is not None for s in eval_scores) else 0\n",
    "        eval_count = len(eval_xs) \n",
    "        \n",
    "        return float(avg_score), eval_count\n",
    "\n",
    "    def _calculate_ucb(self, candidate_buffer_entry: Dict, total_tracked_evaluations: int) -> float:\n",
    "        \"\"\"Calculates UCB score for a candidate in the buffer.\"\"\"\n",
    "        if candidate_buffer_entry['eval_count'] == 0:\n",
    "            return float('inf') \n",
    "        \n",
    "        mean_score = candidate_buffer_entry['score_sum'] / candidate_buffer_entry['eval_count']\n",
    "        \n",
    "        if total_tracked_evaluations == 0: \n",
    "             total_tracked_evaluations = 1\n",
    "        \n",
    "        exploration_term = self.ucb_exploration_factor * \\\n",
    "                           math.sqrt(math.log(total_tracked_evaluations + 1e-9) / candidate_buffer_entry['eval_count'])\n",
    "        \n",
    "        return mean_score + exploration_term\n",
    "    \n",
    "    def _calculate_lcb(self, candidate_buffer_entry: Dict, total_tracked_evaluations: int) -> float:\n",
    "        \"\"\"Calculates Lower Confidence Bound for a candidate in the buffer.\"\"\"\n",
    "        if candidate_buffer_entry['eval_count'] == 0:\n",
    "            return float('-inf')  # Unvisited states get lowest bound\n",
    "        \n",
    "        mean_score = candidate_buffer_entry['score_sum'] / candidate_buffer_entry['eval_count']\n",
    "        \n",
    "        # Add 1 to total_tracked_evaluations to prevent log(0) if it's the first evaluation overall\n",
    "        # and to ensure log argument is > 0.\n",
    "        # Add 1 to eval_count in denominator as well to ensure it's robust if eval_count is small.\n",
    "        if total_tracked_evaluations == 0: # Should not happen if we init with one eval\n",
    "             total_tracked_evaluations = 1\n",
    "        \n",
    "        # LCB exploration term: ucb_exploration_factor scales the confidence interval\n",
    "        # Higher factor = more exploration, lower factor = more exploitation\n",
    "        exploration_term = self.ucb_exploration_factor * \\\n",
    "                           math.sqrt(math.log(total_tracked_evaluations) / candidate_buffer_entry['eval_count'])\n",
    "        \n",
    "        return mean_score - exploration_term\n",
    "    \n",
    "    def _update_buffer_ucb_scores(self):\n",
    "        \"\"\"Recalculates and updates UCB scores for all candidates in the buffer.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "        \n",
    "        for candidate_entry in self.buffer:\n",
    "            candidate_entry['ucb_score'] = self._calculate_ucb(candidate_entry, self._total_evaluations_tracker)\n",
    "\n",
    "    def _get_best_candidate_from_buffer(self, buffer):\n",
    "        \"\"\"Get the best candidate from buffer, excluding those with eval_count = 0.\"\"\"\n",
    "        if not buffer:\n",
    "            return None\n",
    "        \n",
    "        # Filter out candidates with eval_count = 0 \n",
    "        valid_candidates = [c for c in buffer if c['eval_count'] > 0]\n",
    "        if not valid_candidates:\n",
    "            # If no candidates have been evaluated, return the one with highest UCB score\n",
    "            return max(buffer, key=lambda c: c.get('ucb_score', -float('inf')))\n",
    "        return max(valid_candidates, key=lambda c: c['score_sum'] / c['eval_count'])\n",
    "    \n",
    "    def print_intervals(self, buffer):\n",
    "        \"\"\"Print confidence intervals for debugging in the form of open intervals (LCB, UCB)\"\"\"\n",
    "        print_color(\"Confidence intervals for all candidates:\", 'cyan')\n",
    "        for i, candidate_entry in enumerate(buffer):\n",
    "            lcb = self._calculate_lcb(candidate_entry, self._total_evaluations_tracker)\n",
    "            ucb = candidate_entry['ucb_score']\n",
    "            mean_score = candidate_entry['score_sum'] / (candidate_entry['eval_count'] or 1)\n",
    "            eval_count = candidate_entry['eval_count']\n",
    "            \n",
    "            # Format as open interval (LCB, UCB) with mean score and evaluation count\n",
    "            interval_str = f\"Action {i+1}: ({lcb:.4f}, {ucb:.4f}) [mean: {mean_score:.4f}, n: {eval_count}]\"\n",
    "            print_color(interval_str, 'cyan')\n",
    "\n",
    "    def _llm_generate_candidate(self) -> Optional[Dict[trace.nodes.ParameterNode, str]]:\n",
    "        \"\"\"\n",
    "        Prompts an LLM with current buffer candidates to generate new string values for parameters.\n",
    "        Returns a dictionary mapping ParameterNode objects to new string values, or None on failure.\n",
    "        \"\"\"\n",
    "        print_color(\"Attempting to generate candidate using LLM...\", \"blue\")\n",
    "        if not self.buffer:\n",
    "            print_color(\"LLM generation: Buffer is empty, cannot provide context to LLM.\", \"yellow\")\n",
    "            return None\n",
    "\n",
    "        sorted_buffer = sorted(list(self.buffer), key=lambda c: c.get('ucb_score', -float('inf')), reverse=True)\n",
    "        # Include first, last, and evenly spaced middle candidates\n",
    "        if len(sorted_buffer) <= self.num_samples_in_prompt:\n",
    "            prompt_candidates = sorted_buffer\n",
    "        elif self.num_samples_in_prompt <= 2:\n",
    "            # If only 1-2 samples requested, take first and optionally last\n",
    "            prompt_candidates = sorted_buffer[:self.num_samples_in_prompt]\n",
    "        else:\n",
    "            # Take first, last, and evenly spaced middle candidates\n",
    "            prompt_candidates = [sorted_buffer[0]]  # First (highest UCB)\n",
    "            if self.num_samples_in_prompt > 2:\n",
    "                # Calculate indices for middle candidates\n",
    "                middle_count = self.num_samples_in_prompt - 2  # Exclude first and last\n",
    "                if middle_count > 0 and len(sorted_buffer) > 2:\n",
    "                    # Evenly space middle candidates between index 1 and len-2\n",
    "                    middle_indices = [int(1 + i * (len(sorted_buffer) - 2) / (middle_count + 1)) \n",
    "                                    for i in range(1, middle_count + 1)]\n",
    "                    prompt_candidates.extend([sorted_buffer[i] for i in middle_indices])\n",
    "            prompt_candidates.append(sorted_buffer[-1])  # Last (lowest UCB)\n",
    "        \n",
    "        serializable_candidate_summaries = []\n",
    "        for cand_entry in prompt_candidates:\n",
    "            summary = {\n",
    "                \"parameters\":  {getattr(p,'py_name'): copy.deepcopy(p.data) for p in cand_entry['params']},\n",
    "                \"eval_count\": cand_entry['eval_count'],\n",
    "                \"ucb_score\": round(cand_entry.get('ucb_score',0), 4),\n",
    "            }\n",
    "            serializable_candidate_summaries.append(summary)\n",
    "        \n",
    "        example_param_structure_json_str = {getattr(p,'py_name'): copy.deepcopy(p.data) for p in self.agent.parameters()}\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in model optimization. Your task is to propose new string values for model parameters with high UCB scores. Please output ONLY a valid JSON dictionary where keys are parameter names and values are the new string values for those parameters, matching the example structure provided. Do not add any explanations or markdown formatting around the JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Here are some current candidates from the search buffer and their statistics:\\\\n{serializable_candidate_summaries}\\\\n\\\\nHere is an example of the required JSON output structure (parameter names as keys, new string values as values):\\\\n{example_param_structure_json_str}\\\\n\\\\nPlease generate a new set of parameters in exactly the same JSON format. Make sure use double quotes for the keys and values.\"}\n",
    "        ]\n",
    "        \n",
    "        print_color(f\"LLM prompt (summary): {len(prompt_candidates)} candidates, structure example provided.\", \"magenta\")\n",
    "        response_format =  {\"type\": \"json_object\"}\n",
    "        llm_response = self.llm(prompt_messages, response_format=response_format) \n",
    "        llm_response_str = llm_response.choices[0].message.content\n",
    "\n",
    "        if not llm_response_str:\n",
    "            print_color(\"LLM returned an empty response.\", \"red\")\n",
    "            return None\n",
    "        \n",
    "        cleaned_llm_response_str = llm_response_str.strip()\n",
    "\n",
    "        try:\n",
    "            llm_params_raw = json.loads(cleaned_llm_response_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print_color(f\"JSON parsing attempts failed: {e}\", \"red\")\n",
    "            print_color(\"Returning the candidate with the highest UCB score in the buffer.\", \"red\")\n",
    "            return max(self.buffer, key=lambda c: c.get('ucb_score', -float('inf')))['params']\n",
    "\n",
    "        if not isinstance(llm_params_raw, dict):\n",
    "            print_color(f\"LLM output was not a JSON dictionary after parsing: {type(llm_params_raw)}\", \"red\")\n",
    "            print_color(\"Returning the candidate with the highest UCB score in the buffer.\", \"red\")\n",
    "            return max(self.buffer, key=lambda c: c.get('ucb_score', -float('inf')))['params']\n",
    "\n",
    "        candidate_params_dict = self.construct_update_dict(llm_params_raw)\n",
    "        return candidate_params_dict\n",
    "    \n",
    "    def construct_update_dict(self, suggestion: Dict[str, Any]) -> Dict[ParameterNode, Any]:\n",
    "        \"\"\"Convert the suggestion in text into the right data type.\"\"\"\n",
    "        update_dict = {}\n",
    "        for node in self.agent.parameters():\n",
    "            if node.trainable and node.py_name in suggestion:\n",
    "                try:\n",
    "                    formatted_suggestion = suggestion[node.py_name]\n",
    "                    if type(formatted_suggestion) == str and 'def' in formatted_suggestion:\n",
    "                        formatted_suggestion = format_str(formatted_suggestion, mode=FileMode())\n",
    "                    update_dict[node] = type(node.data)(formatted_suggestion)\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    if getattr(self, 'ignore_extraction_error', False):\n",
    "                        warnings.warn(\n",
    "                            f\"Cannot convert the suggestion '{suggestion[node.py_name]}' for {node.py_name} to the right data type\"\n",
    "                        )\n",
    "                    else:\n",
    "                        raise e\n",
    "        return update_dict\n",
    "\n",
    "    def train(self,\n",
    "              guide, \n",
    "              train_dataset: Dict[str, List[Any]],\n",
    "              *,\n",
    "              num_search_iterations: int = 100,\n",
    "              validation_dataset: Dict[str, List[Any]] = None,\n",
    "              test_dataset: Dict[str, List[Any]] = None,\n",
    "              train_batch_size: int = 5, \n",
    "              evaluation_batch_size: int = 5,\n",
    "              eval_frequency: int = 1, \n",
    "              log_frequency: Optional[int] = None,\n",
    "              save_frequency: Optional[int] = None,\n",
    "              save_path: str = \"checkpoints/ucb_llm_agent.pkl\",\n",
    "              min_score_for_agent_update: Optional[float] = None,\n",
    "              verbose: Union[bool, str] = False,\n",
    "              num_threads: Optional[int] = None,\n",
    "              print_confidence_interval: bool = True,\n",
    "              **kwargs\n",
    "              ) -> Tuple[Dict[str, Any], float]:\n",
    "        \n",
    "        if validation_dataset is None:\n",
    "            validation_dataset = train_dataset\n",
    "        if test_dataset is None:\n",
    "            test_dataset = train_dataset\n",
    "\n",
    "        num_threads = num_threads or self.num_threads\n",
    "        log_frequency = log_frequency or eval_frequency\n",
    "        self.min_score = min_score_for_agent_update \n",
    "        total_samples = 0\n",
    "        self.total_proposals = 0\n",
    "\n",
    "        metrics = {\n",
    "            'best_candidate_scores': [], \n",
    "            'selected_action_ucb': [],\n",
    "            'new_candidate_scores': [], \n",
    "            'buffer_avg_score': [],\n",
    "            'buffer_avg_evals': [],\n",
    "            'llm_generation_failures': 0,\n",
    "            'generation_path': []\n",
    "        }\n",
    "\n",
    "        # Initial candidate evaluation\n",
    "        print_color(\"Evaluating initial parameters using train_dataset samples...\", 'cyan')\n",
    "        initial_params_dict = {p: copy.deepcopy(p.data) for p in self.agent.parameters()}\n",
    "         \n",
    "        initial_score, initial_evals = self._evaluate_candidate(\n",
    "            initial_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "        )\n",
    "        self._total_evaluations_tracker += initial_evals \n",
    "        total_samples += initial_evals\n",
    "\n",
    "        initial_candidate_entry = {\n",
    "            'params': initial_params_dict,\n",
    "            'score_sum': initial_score * initial_evals if initial_score > -np.inf else 0,\n",
    "            'eval_count': initial_evals,\n",
    "            'ucb_score': 0.0, \n",
    "            'iteration_created': 0\n",
    "        }\n",
    "        self.buffer.append(initial_candidate_entry)\n",
    "        self._update_buffer_ucb_scores() \n",
    "        print_color(f\"Initial candidate: Score {initial_score:.4f}, Evals {initial_evals}\", 'yellow')\n",
    "        \n",
    "        # Log initial evaluation\n",
    "        self.logger.log('Initial UCB score', initial_score, 0, color='blue')\n",
    "        self.logger.log('Total samples', total_samples, 0, color='cyan')\n",
    "        self.logger.log('Total proposals', self.total_proposals, 0, color='red')\n",
    "        \n",
    "        # Main search loop\n",
    "        for iteration in range(1, num_search_iterations + 1):\n",
    "            try:\n",
    "                if not self.buffer:\n",
    "                    print_color(\"Buffer is empty, stopping search.\", 'red')\n",
    "                    break\n",
    "\n",
    "                self._update_buffer_ucb_scores()\n",
    "                a_prime_params_dict = None\n",
    "                a_prime_score = 0\n",
    "                a_prime_evals = 0\n",
    "                generation_method = \"none\"\n",
    "                if print_confidence_interval:\n",
    "                    self.print_intervals(self.buffer)\n",
    "\n",
    "                if iteration<=2 or random.random() < self.alpha: # UCB Path, for the first 2 iterations, we always use UCB because the buffer size is small, it's hard for LLM to generate good candidates\n",
    "                    generation_method = \"ucb\"\n",
    "                    metrics['generation_path'].append(\"ucb\")\n",
    "                    if not self.buffer:\n",
    "                        print_color(f\"Iter {iteration} (UCB Path): Buffer empty, cannot select action. Skipping.\", \"red\")\n",
    "                        continue\n",
    "                    \n",
    "                    action_candidate_a = self.select(self.buffer)\n",
    "                    \n",
    "                    selected_mean_score = action_candidate_a['score_sum'] / action_candidate_a['eval_count'] if action_candidate_a['eval_count'] > 0 else -np.inf\n",
    "                    print_color(f\"Iter {iteration} (UCB Path): Selected action candidate (UCB: {action_candidate_a['ucb_score']:.4f}, MeanScore: {selected_mean_score:.4f} Evals: {action_candidate_a['eval_count']})\", 'blue')\n",
    "                    # metrics['selected_action_ucb'].append(action_candidate_a['ucb_score'])\n",
    "                    \n",
    "                    # Log selected action UCB score\n",
    "                    # self.logger.log('Selected action UCB', action_candidate_a['ucb_score'], iteration, color='magenta')\n",
    "                    # self.logger.log('Selected action mean score', selected_mean_score, iteration, color='cyan')\n",
    "\n",
    "                    self.optimizer.update(action_candidate_a['params'])\n",
    "\n",
    "                    train_xs, train_infos = self._sample_minibatch(train_dataset, train_batch_size)\n",
    "                    if not train_xs:\n",
    "                        print_color(f\"Iter {iteration} (UCB Path): Training minibatch empty, skipping optimizer step.\", 'yellow')\n",
    "                        continue \n",
    "                    \n",
    "                    total_samples += len(train_xs)\n",
    "\n",
    "                    # Forward pass for 'a'\n",
    "                    outputs_for_a = []\n",
    "                    use_asyncio = self._use_asyncio(num_threads)\n",
    "                    if use_asyncio:\n",
    "                        outputs_for_a = async_run([self.forward]*len(train_xs),\n",
    "                                           [(self.agent, x, guide, info) for x, info in zip(train_xs, train_infos)],\n",
    "                                           max_workers=num_threads,\n",
    "                                           description=f\"Iter {iteration} (UCB): Forward for 'a'\")\n",
    "                    else:\n",
    "                        outputs_for_a = [self.forward(self.agent, x, guide, info) for x, info in zip(train_xs, train_infos)]\n",
    "\n",
    "                    scores_from_train, targets_from_train, feedbacks_from_train = [], [], []\n",
    "                    for target, score, feedback in outputs_for_a:\n",
    "                        scores_from_train.append(score)\n",
    "                        targets_from_train.append(target)\n",
    "                        feedbacks_from_train.append(feedback)\n",
    "                    \n",
    "                    if not scores_from_train:\n",
    "                        print_color(f\"Iter {iteration} (UCB Path): No outputs from forward pass for 'a'. Skipping.\", 'yellow')\n",
    "                        continue\n",
    "\n",
    "                    target_for_a = batchify(*targets_from_train)\n",
    "                    feedback_for_a = batchify(*feedbacks_from_train).data\n",
    "                    score_for_a_on_train_batch = np.mean([s for s in scores_from_train if s is not None]) if any(s is not None for s in scores_from_train) else 0\n",
    "\n",
    "                    self.optimizer.zero_feedback()\n",
    "                    self.optimizer.backward(target_for_a, feedback_for_a)\n",
    "\n",
    "                    # Get a_prime by optimizer step\n",
    "                    try:\n",
    "                        returned_params = self.optimizer.step(bypassing=True, verbose=False) \n",
    "                        if not isinstance(returned_params, dict) or not returned_params:\n",
    "                            print_color(f\"Iter {iteration} (UCB Path): Optimizer.step did not return a valid param dict for a_prime. Using current agent params.\", 'yellow')\n",
    "                            a_prime_params_dict = {p: copy.deepcopy(p.data) for p in self.agent.parameters()}\n",
    "                        else:\n",
    "                            a_prime_params_dict = {p: copy.deepcopy(p.data)  for p in returned_params}\n",
    "                        self.total_proposals += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration} (UCB Path): Error during optimizer.step for a_prime: {e}. Skipping.\", 'red')\n",
    "                        continue\n",
    "                    \n",
    "                    # Evaluate 'a' and 'a_prime' on validation set in parallel (like UCBSearchAlgorithm)\n",
    "                    use_asyncio = self._use_asyncio(num_threads)\n",
    "                    if use_asyncio:\n",
    "                        evaluation_results = async_run(\n",
    "                            [self._evaluate_candidate, self._evaluate_candidate],\n",
    "                            [\n",
    "                                (action_candidate_a['params'], validation_dataset, guide, evaluation_batch_size, num_threads),\n",
    "                                (a_prime_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads)\n",
    "                            ],\n",
    "                            max_workers=2,\n",
    "                            description=f\"Iter {iteration} (UCB): Parallel evaluation of 'a' and 'a_prime'\"\n",
    "                        )\n",
    "                        (a_score, a_evals), (a_prime_score, a_prime_evals) = evaluation_results\n",
    "                    else:\n",
    "                        a_score, a_evals = self._evaluate_candidate(\n",
    "                            action_candidate_a['params'], validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "                        )\n",
    "                        a_prime_score, a_prime_evals = self._evaluate_candidate(\n",
    "                            a_prime_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "                        )\n",
    "                    \n",
    "                    self._total_evaluations_tracker += a_evals + a_prime_evals\n",
    "                    total_samples += a_evals + a_prime_evals\n",
    "\n",
    "                    # Update stats of action_candidate_a\n",
    "                    if score_for_a_on_train_batch > -np.inf:\n",
    "                        action_candidate_a['score_sum'] += score_for_a_on_train_batch * len(train_xs)\n",
    "                        action_candidate_a['eval_count'] += len(train_xs)\n",
    "                        self._total_evaluations_tracker += len(train_xs)\n",
    "                    \n",
    "                    # Update stats with validation evaluation of 'a'\n",
    "                    action_candidate_a['score_sum'] += a_score * a_evals\n",
    "                    action_candidate_a['eval_count'] += a_evals\n",
    "                    \n",
    "                    print_color(f\"Iter {iteration} (UCB Path): New candidate a_prime (from UCB) generated. Eval Score: {a_prime_score:.4f}, Evals: {a_prime_evals}\", 'cyan')\n",
    "                    self.logger.log('New candidate score', a_prime_score, iteration, color='green') \n",
    "                    self.logger.log('Training batch score', score_for_a_on_train_batch, iteration, color='yellow')\n",
    "                else: # LLM Pathcandi\n",
    "                    generation_method = \"llm\"\n",
    "                    metrics['generation_path'].append(\"llm\")\n",
    "                    print_color(f\"Iter {iteration} (LLM Path): Generating candidate via LLM.\", 'blue')\n",
    "                    a_prime_params_dict = self._llm_generate_candidate()\n",
    "\n",
    "                    if a_prime_params_dict:\n",
    "                        # Evaluate a_prime (from LLM path)\n",
    "                        a_prime_score, a_prime_evals = self._evaluate_candidate(\n",
    "                            a_prime_params_dict, validation_dataset, guide, evaluation_batch_size, num_threads\n",
    "                        )\n",
    "                        self._total_evaluations_tracker += a_prime_evals\n",
    "                        total_samples += a_prime_evals\n",
    "                        self.total_proposals += 1\n",
    "                        print_color(f\"Iter {iteration} (LLM Path): New candidate a_prime (from LLM) generated. Eval Score: {a_prime_score:.4f}, Evals: {a_prime_evals}\", 'cyan')\n",
    "                        self.logger.log('New candidate score', a_prime_score, iteration, color='green') #average new candidate score\n",
    "                    else:\n",
    "                        print_color(f\"Iter {iteration} (LLM Path): LLM failed to generate a valid candidate. Skipping addition to buffer.\", 'red')\n",
    "                        metrics['llm_generation_failures'] += 1\n",
    "                        continue\n",
    "\n",
    "                # Common logic for adding a_prime to buffer\n",
    "                metrics['new_candidate_scores'].append(a_prime_score)\n",
    "\n",
    "                if a_prime_params_dict and a_prime_score > -np.inf and a_prime_evals > 0:\n",
    "                    new_candidate_entry = {\n",
    "                        'params': a_prime_params_dict,\n",
    "                        'score_sum': a_prime_score * a_prime_evals,\n",
    "                        'eval_count': a_prime_evals,\n",
    "                        'ucb_score': 0.0, \n",
    "                        'iteration_created': iteration\n",
    "                    }\n",
    "                    \n",
    "                    if len(self.buffer) == self.max_buffer_size:\n",
    "                        self._update_buffer_ucb_scores()\n",
    "                        candidate_to_evict = min(self.buffer, key=lambda c: c['ucb_score'])\n",
    "                        self.buffer.remove(candidate_to_evict)\n",
    "                        evicted_mean_score = candidate_to_evict['score_sum'] / candidate_to_evict['eval_count'] if candidate_to_evict['eval_count'] > 0 else -np.inf\n",
    "                        print_color(f\"Iter {iteration}: Buffer full. Evicted candidate (UCB: {candidate_to_evict['ucb_score']:.4f}, MeanScore: {evicted_mean_score:.4f})\", 'magenta')\n",
    "                    \n",
    "                    self.buffer.append(new_candidate_entry)\n",
    "                    print_color(f\"Iter {iteration}: Added new candidate (from {generation_method}) to buffer.\", 'magenta')\n",
    "                elif a_prime_params_dict:\n",
    "                    print_color(f\"Iter {iteration}: New candidate a_prime (from {generation_method}) had invalid score/evals ({a_prime_score}, {a_prime_evals}), not added to buffer.\", 'yellow')\n",
    "\n",
    "                self._update_buffer_ucb_scores()\n",
    "\n",
    "                # Logging\n",
    "                if self.buffer:\n",
    "                    best_in_buffer = max(self.buffer, key=lambda c: (c['score_sum']/(c['eval_count'] if c['eval_count'] > 0 else 1)))\n",
    "                    current_best_score = best_in_buffer['score_sum']/(best_in_buffer['eval_count'] if best_in_buffer['eval_count'] > 0 else 1)\n",
    "                    metrics['best_candidate_scores'].append(current_best_score)\n",
    "                    \n",
    "                    valid_scores = [c['score_sum']/(c['eval_count'] if c['eval_count'] > 0 else 1) for c in self.buffer if c['eval_count'] > 0]\n",
    "                    metrics['buffer_avg_score'].append(np.mean(valid_scores) if valid_scores else -np.inf)\n",
    "                    metrics['buffer_avg_evals'].append(np.mean([c['eval_count'] for c in self.buffer]))\n",
    "                else:\n",
    "                    metrics['best_candidate_scores'].append(0)\n",
    "                    metrics['buffer_avg_score'].append(0)\n",
    "                    metrics['buffer_avg_evals'].append(0)\n",
    "\n",
    "                if iteration % log_frequency == 0:\n",
    "                    log_data = {\n",
    "                        \"iteration\": iteration,\n",
    "                        \"best_score\": metrics['best_candidate_scores'][-1],\n",
    "                        \"newly_evaluated_candidate_score\": a_prime_score,\n",
    "                        \"buffer_size\": len(self.buffer),\n",
    "                        \"buffer_avg_score\": metrics['buffer_avg_score'][-1],\n",
    "                        \"buffer_avg_evals\": metrics['buffer_avg_evals'][-1],\n",
    "                        \"total_evaluations_ucb_T\": self._total_evaluations_tracker,\n",
    "                        \"total_samples\": total_samples,\n",
    "                        \"generation_method_this_iter\": generation_method,\n",
    "                        \"llm_generation_total_failures\": metrics['llm_generation_failures']\n",
    "                    }\n",
    "                    if generation_method == \"ucb\" and metrics['selected_action_ucb']:\n",
    "                        log_data[\"selected_action_ucb\"] = metrics['selected_action_ucb'][-1]\n",
    "                    \n",
    "                    # Log all important metrics\n",
    "                    self.logger.log('Best candidate score', log_data['best_score'], iteration, color='green')\n",
    "                    self.logger.log('Buffer size', log_data['buffer_size'], iteration, color='blue')\n",
    "                    self.logger.log('Buffer average score', log_data['buffer_avg_score'], iteration, color='cyan')\n",
    "                    self.logger.log('Buffer average evaluations', log_data['buffer_avg_evals'], iteration, color='orange')\n",
    "                    self.logger.log('Total samples', log_data['total_samples'], iteration, color='yellow')\n",
    "                    self.logger.log('Total proposals', self.total_proposals, iteration, color='red')\n",
    "                    \n",
    "                    print_color(f\"Log @ Iter {iteration}: Best score in buffer: {log_data['best_score']:.4f}, Gen method: {generation_method}, Buffer size: {len(self.buffer)}, Total samples: {total_samples}\", 'green')\n",
    "\n",
    "                if test_dataset is not None and iteration % eval_frequency == 0:\n",
    "                    try:\n",
    "                        # Save current agent parameters\n",
    "                        current_params = {p: copy.deepcopy(p.data) for p in self.optimizer.parameters}\n",
    "                        \n",
    "                        # Find the best candidate in the buffer (highest mean score)\n",
    "                        best_candidate = self._get_best_candidate_from_buffer(self.buffer)\n",
    "                        if not best_candidate:\n",
    "                            print_color(f\"Iter {iteration}: No valid candidate for test evaluation.\", 'yellow')\n",
    "                            continue\n",
    "                        \n",
    "                        # Load best candidate's parameters into the agent for evaluation\n",
    "                        self.optimizer.update(best_candidate['params'])\n",
    "                        \n",
    "                        # Evaluate the best candidate on test set\n",
    "                        test_score = self.evaluate(self.agent, guide, test_dataset['inputs'], test_dataset['infos'],\n",
    "                                      min_score=self.min_score, num_threads=num_threads,\n",
    "                                      description=f\"Evaluating best candidate (iteration {iteration})\")\n",
    "                        \n",
    "                        # Restore original agent parameters\n",
    "                        self.optimizer.update(current_params)\n",
    "                        \n",
    "                        self.logger.log('Test score', test_score, iteration, color='green')\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration}: Test evaluation failed: {e}\", 'red')\n",
    "                \n",
    "                if save_frequency is not None and iteration % save_frequency == 0 and self.buffer:\n",
    "                    try:\n",
    "                        best_overall_candidate_entry = max(self.buffer, key=lambda c: (c['score_sum'] / (c['eval_count'] if c['eval_count'] > 0 else 1E-9)))\n",
    "                        self.optimizer.update(best_overall_candidate_entry['params']) \n",
    "                        if hasattr(self, 'save_agent'):\n",
    "                            self.save_agent(save_path, iteration) \n",
    "                            best_mean_score_for_save = best_overall_candidate_entry['score_sum'] / (best_overall_candidate_entry['eval_count'] if best_overall_candidate_entry['eval_count'] > 0 else 1E-9)\n",
    "                            print_color(f\"Iter {iteration}: Saved agent based on best candidate in buffer (Mean Score: {best_mean_score_for_save:.4f}).\", 'green')\n",
    "                        else:\n",
    "                            print_color(f\"Iter {iteration}: save_agent method not found, skipping save.\", 'yellow')\n",
    "                    except Exception as e:\n",
    "                        print_color(f\"Iter {iteration}: Agent save failed: {e}\", 'red')\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print_color(f\"Iter {iteration}: Iteration failed with error: {e}. Skipping to next iteration.\", 'red')\n",
    "                self.logger.log('Iteration error', str(e), iteration, color='red')\n",
    "                continue\n",
    "\n",
    "        print_color(\"UCB-LLM search finished.\", 'blue')\n",
    "                    \n",
    "        final_best_candidate = max(self.buffer, key=lambda c: (c['score_sum'] / (c['eval_count'] if c['eval_count'] > 0 else 1E-9)))\n",
    "        final_best_score = final_best_candidate['score_sum'] / (final_best_candidate['eval_count'] if final_best_candidate['eval_count'] > 0 else 1E-9)\n",
    "        final_best_evals = final_best_candidate['eval_count']\n",
    "        print_color(f\"Final best candidate: Mean Score {final_best_score:.4f}, Evals {final_best_evals}\", 'green')\n",
    "\n",
    "        self.optimizer.update(final_best_candidate['params'])\n",
    "\n",
    "        return metrics, float(final_best_score)\n",
    "    \n",
    "    def select(self, buffer):\n",
    "        '''Selects candidate with highest UCB score.'''\n",
    "        if not buffer: return None\n",
    "        return max(buffer, key=lambda c: c.get('ucb_score', -float('inf')))\n",
    "\n",
    "\n",
    "class UCBSearchFunctionApproximationAlgorithm(UCBSearchAlgorithm):\n",
    "    \"\"\"\n",
    "    UCB Search Algorithm that uses LLM function approximation to select candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model,num_samples_in_prompt:int=5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.llm_model = llm_model\n",
    "        self.llm = LLM(model=self.llm_model)\n",
    "        self.num_samples_in_prompt = num_samples_in_prompt\n",
    "        print_color(f\"Initialized UCBSearchFunctionApproximationAlgorithm with LLM model={self.llm_model}\", \"cyan\")\n",
    "    \n",
    "    def select(self, buffer): \n",
    "        \"\"\"Generate a new candidate entry using LLM. Note: this doesn't add it to the buffer.\"\"\"\n",
    "        new_action_params = self._llm_generate_candidate()\n",
    "        new_candidate_entry = {\n",
    "            'params': new_action_params,\n",
    "            'score_sum': 0,\n",
    "            'eval_count': 0,\n",
    "            'ucb_score': 0.0, \n",
    "            'iteration_created': 0\n",
    "        }\n",
    "        return new_candidate_entry\n",
    "    \n",
    "    def _llm_generate_candidate(self) -> Optional[Dict[trace.nodes.ParameterNode, str]]:\n",
    "        \"\"\"\n",
    "        Prompts an LLM with current buffer candidates to generate new string values for parameters.\n",
    "        Returns a dictionary mapping ParameterNode objects to new string values, or None on failure.\n",
    "        \"\"\"\n",
    "        print_color(\"Attempting to generate candidate using LLM...\", \"blue\")\n",
    "        if not self.buffer:\n",
    "            print_color(\"LLM generation: Buffer is empty, cannot provide context to LLM.\", \"yellow\")\n",
    "            return None\n",
    "\n",
    "        sorted_buffer = sorted(list(self.buffer), key=lambda c: c.get('ucb_score', -float('inf')), reverse=True)\n",
    "        # Include first, last, and evenly spaced middle candidates\n",
    "        if len(sorted_buffer) <= self.num_samples_in_prompt:\n",
    "            prompt_candidates = sorted_buffer\n",
    "        elif self.num_samples_in_prompt <= 2:\n",
    "            # If only 1-2 samples requested, take first and optionally last\n",
    "            prompt_candidates = sorted_buffer[:self.num_samples_in_prompt]\n",
    "        else:\n",
    "            # Take first, last, and evenly spaced middle candidates\n",
    "            prompt_candidates = [sorted_buffer[0]]  # First (highest UCB)\n",
    "            if self.num_samples_in_prompt > 2:\n",
    "                # Calculate indices for middle candidates\n",
    "                middle_count = self.num_samples_in_prompt - 2  # Exclude first and last\n",
    "                if middle_count > 0 and len(sorted_buffer) > 2:\n",
    "                    # Evenly space middle candidates between index 1 and len-2\n",
    "                    middle_indices = [int(1 + i * (len(sorted_buffer) - 2) / (middle_count + 1)) \n",
    "                                    for i in range(1, middle_count + 1)]\n",
    "                    prompt_candidates.extend([sorted_buffer[i] for i in middle_indices])\n",
    "            prompt_candidates.append(sorted_buffer[-1])  # Last (lowest UCB)\n",
    "        \n",
    "        serializable_candidate_summaries = []\n",
    "        for cand_entry in prompt_candidates:\n",
    "            summary = {\n",
    "                \"parameters\":  {getattr(p,'py_name'): copy.deepcopy(p.data) for p in cand_entry['params']},\n",
    "                \"eval_count\": cand_entry['eval_count'],\n",
    "                \"ucb_score\": round(cand_entry.get('ucb_score',0), 4),\n",
    "            }\n",
    "            serializable_candidate_summaries.append(summary)\n",
    "        \n",
    "        example_param_structure_json_str = {getattr(p,'py_name'): copy.deepcopy(p.data) for p in self.agent.parameters()}\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in model optimization. Your task is to propose new string values for model parameters with high UCB scores. Please output ONLY a valid JSON dictionary where keys are parameter names and values are the new string values for those parameters, matching the example structure provided. Do not add any explanations or markdown formatting around the JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Here are some current candidates from the search buffer and their statistics:\\\\n{serializable_candidate_summaries}\\\\n\\\\nHere is an example of the required JSON output structure (parameter names as keys, new string values as values):\\\\n{example_param_structure_json_str}\\\\n\\\\nPlease generate a new set of parameters in exactly the same JSON format. Make sure use double quotes for the keys and values.\"}\n",
    "        ]\n",
    "        \n",
    "        print_color(f\"LLM prompt (summary): {len(prompt_candidates)} candidates, structure example provided.\", \"magenta\")\n",
    "        response_format =  {\"type\": \"json_object\"}\n",
    "        llm_response = self.llm(prompt_messages, response_format=response_format) \n",
    "        llm_response_str = llm_response.choices[0].message.content\n",
    "\n",
    "        if not llm_response_str:\n",
    "            print_color(\"LLM returned an empty response.\", \"red\")\n",
    "            return None\n",
    "        \n",
    "        cleaned_llm_response_str = llm_response_str.strip()\n",
    "\n",
    "        try:\n",
    "            llm_params_raw = json.loads(cleaned_llm_response_str)\n",
    "            self.total_proposals += 1\n",
    "        except json.JSONDecodeError as e:\n",
    "            print_color(f\"JSON parsing attempts failed: {e}\", \"red\")\n",
    "            print_color(\"Returning the candidate with the highest UCB score in the buffer.\", \"red\")\n",
    "            return max(self.buffer, key=lambda c: c.get('ucb_score', -float('inf')))['params']\n",
    "\n",
    "        if not isinstance(llm_params_raw, dict):\n",
    "            print_color(f\"LLM output was not a JSON dictionary after parsing: {type(llm_params_raw)}\", \"red\")\n",
    "            print_color(\"Returning the candidate with the highest UCB score in the buffer.\", \"red\")\n",
    "            return max(self.buffer, key=lambda c: c.get('ucb_score', -float('inf')))['params']\n",
    "\n",
    "        candidate_params_dict = self.construct_update_dict(llm_params_raw)\n",
    "        return candidate_params_dict\n",
    "    \n",
    "    def construct_update_dict(self, suggestion: Dict[str, Any]) -> Dict[ParameterNode, Any]:\n",
    "        \"\"\"Convert the suggestion in text into the right data type.\"\"\"\n",
    "        update_dict = {}\n",
    "        for node in self.agent.parameters():\n",
    "            if node.trainable and node.py_name in suggestion:\n",
    "                try:\n",
    "                    formatted_suggestion = suggestion[node.py_name]\n",
    "                    if type(formatted_suggestion) == str and 'def' in formatted_suggestion:\n",
    "                        formatted_suggestion = format_str(formatted_suggestion, mode=FileMode())\n",
    "                    update_dict[node] = type(node.data)(formatted_suggestion)\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    if getattr(self, 'ignore_extraction_error', False):\n",
    "                        warnings.warn(\n",
    "                            f\"Cannot convert the suggestion '{suggestion[node.py_name]}' for {node.py_name} to the right data type\"\n",
    "                        )\n",
    "                    else:\n",
    "                        raise e\n",
    "        return update_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ba126-93dc-4f73-a9f4-b8bcd7120315",
   "metadata": {},
   "source": [
    "## Integrate Search Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe432a67-e0e8-4a29-8cb3-4640e8cc5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "math_data = datasets.load_dataset('xuanfeiren/math_hard_gemini')\n",
    "\n",
    "num_train_samples = 1\n",
    "num_validate_samples = 1\n",
    "num_test_samples = 1\n",
    "\n",
    "# Select data subsets\n",
    "train_data = math_data['train'].select(\n",
    "    range(num_train_samples, num_train_samples + num_validate_samples)\n",
    ")\n",
    "validate_data = train_data\n",
    "test_data = math_data['test'].select(range(num_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "962eac97-ed52-4b03-8fd6-c6833fd86c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = \"gpt-4o\"\n",
    "\n",
    "student_llm = LLM()\n",
    "agent = Learner(llm=student_llm)\n",
    "\n",
    "train_guide = TeacherGuide(model=teacher_model)\n",
    "validate_guide = TeacherGuide(model=teacher_model)\n",
    "\n",
    "optimizer = OptoPrimeV2(agent.parameters())\n",
    "logger = SimpleLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7faaee-a88e-492b-937b-184d4f9c44a5",
   "metadata": {},
   "source": [
    "### Beam Search Demo\n",
    "\n",
    "Beam Search as a type of branching and selecting approach: with just constant top-k selection\n",
    "\n",
    "![img](https://d2l.ai/_images/beam-search.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41c1e281-3d18-477f-a274-bfddb7402ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = BeamsearchAlgorithm(\n",
    "            agent=agent,\n",
    "            optimizer=optimizer,\n",
    "            logger=logger,\n",
    "            num_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca48bc9e-60a4-4edf-bf86-61e27f59d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_threads = 1\n",
    "batch_size = 1\n",
    "eval_frequency = 1\n",
    "log_frequency = 1\n",
    "validation_dataset_size = 1\n",
    "\n",
    "# Select data subsets\n",
    "train_data = math_data['train'].select(\n",
    "    range(num_train_samples, num_train_samples + num_validate_samples)\n",
    ")\n",
    "validate_data = train_data\n",
    "test_data = math_data['test'].select(range(num_test_samples))\n",
    "\n",
    "train_dataset = {'inputs': train_data['problem'], 'infos': train_data['solution']}\n",
    "validate_dataset = {'inputs': validate_data['problem'], 'infos': validate_data['solution']}\n",
    "test_dataset = {'inputs': test_data['problem'], 'infos': test_data['solution']}\n",
    "\n",
    "train_params = {\n",
    "    \"guide\": train_guide,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"num_threads\": num_threads,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"test_dataset\": test_dataset,\n",
    "    \"validate_dataset\": validate_dataset,\n",
    "    \"validate_guide\": validate_guide,\n",
    "    \"eval_frequency\": eval_frequency,\n",
    "    \"log_frequency\": log_frequency,\n",
    "    \"validation_dataset_size\": validation_dataset_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7edf775-0987-49b8-8d80-8cd52ba192a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basicsearch_proposals = 5\n",
    "\n",
    "train_params[\"num_proposals\"] = num_basicsearch_proposals\n",
    "\n",
    "# beam search\n",
    "train_params.update({\n",
    "    \"beam_width\": 3,\n",
    "    \"max_depth\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d35f4be0-a367-4951-a4c6-dabebb7ba667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94mRunning BeamsearchAlgorithm with beam_width=3, max_depth=2\u001B[0m\n",
      "\u001B[94mUsing validation_dataset_size=1 for intermediate evaluations\u001B[0m\n",
      "\u001B[94m\n",
      "===== Evaluating Initial Parameters =====\u001B[0m\n",
      "\u001B[93mInitial test score: 0.0000\u001B[0m\n",
      "\u001B[94m\n",
      "===== Beam Search Depth 1/2 with 1 beams =====\u001B[0m\n",
      "\u001B[96mSampled validation minibatch of size 1 for depth 1\u001B[0m\n",
      "\u001B[93mProcessing beam 1/1\u001B[0m\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "The instruction requires us to change the values of `str36` and `str37` to improve the output based on feedback. The issue highlighted in the feedback indicates that the solution generated by the LLM is incorrect primarily because it overcounted the number of ways to color the triangles. The outputs suggest that the initial formulation of the math problem in `str37` may have led to misunderstandings on the part of the LLM, resulting in an incorrect answer. Therefore, making `str37` clearer and more precise can potentially guide the LLM towards deriving the correct approach. Additionally, adjusting `str36` might lead to a better understanding of the context and specific requirements of the task for the LLM.\n",
      "\n",
      "The feedback includes crucial information which can be incorporated into the `str37` to help guide the LLM. The improved template should specify the exact conditions and constraints (e.g., adjacent dots needing different colors) that are crucial for solving the problem accurately.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You are a logical and precise agent, tasked with step-by-step problem solving in mathematics.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem with careful attention to the coloring rules: Each of the nine dots in this figure is to be colored red, white, or blue. No two connected dots can be the same color. Consider how the constraints apply, particularly in adjacent triangles, step-by-step, and calculate the total number of distinct coloring possibilities.\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "The feedback indicates that the output on how the math problem was solved is incorrect due to overcounting the possible colorings for the three triangles. The core problem seems to stem from the logic or process provided within the user prompt and subsequent logic returned by the LLM model. Therefore, updates need to be made to the prompt template `str37` to ensure that the LLM model adopts the correct problem-solving method. The system prompt `str36` could also be enhanced to better frame the expected techniques or approaches to the problem-solving processed by the model. Action to improve output involves ensuring detail on step-by-step verification and constraints on the coloring rules are present in the prompts to achieve the correct model result of 54 ways.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You're a helpful agent focusing on accurate and detailed problem-solving in mathematics, ensuring step-by-step logical reasoning with attention to constraints.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem step-by-step, verifying each step according to constraints: {message}\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "The instruction asks us to adjust the values of the variables to improve the correctness of the output in response to the feedback provided. The feedback indicates that the current solution incorrectly calculates the number of ways to color the dots and provides the correct logical steps to arrive at the correct solution. The `str36` and `str37` variables are responsible for setting the system prompt and the user prompt for the LLM indicating the nature of the problem. The current prompts do not explicitly guide the LLM to use the correct logical steps for solving the problem, as deduced from the feedback.\n",
      "\n",
      "To address the issue, we need to modify the user prompt (`str37`) to explicitly guide the problem-solving process by providing the necessary steps as indicated in the feedback. This guidance will inform the LLM of the correct procedure to follow, correcting the counting logic which was faulty in the initial result. \n",
      "\n",
      "The feedback suggests breaking down the steps, hence we modify `str37` to include these steps explicitly. The system prompt (`str36`) can remain unchanged, as it suitably positions the LLM as a math problem solver.\n",
      "</reasoning>\n",
      "\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem step-by-step: First, color the left-most triangle allowing 6 ways for nonadjacent same-color matching. Then, color the middle triangle with 3 ways taking into consideration the touching base with the first triangle. Finally, color the third triangle, similar to the transition from the first to the second triangle, with 3 ways. Calculate the total combinations accordingly.\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "The #Instruction requires us to modify the variables in #Variables so that the output aligns better with the #Feedback, which indicates the current solution's answer is incorrect. The issue seems to be related to the logic used to solve the math problem, specifically the counting of ways to color the triangles.\n",
      "\n",
      "The #Feedback suggests a detailed method for solving the problem, emphasizing that the student overcounted the permutations. Therefore, it seems the prompt generated for the LLM should be more specific in directing the LLM to follow this logical reasoning about problem-solving steps.\n",
      "\n",
      "The #Variables `str36` and `str37` control the prompts given to the LLM. Adjusting these variables may influence the LLM's interpretation and solution approach to match the #Feedback's method.\n",
      "\n",
      "We'll improve `str37` to encourage a step-by-step solution that focuses on not overcounting methods and dealing specifically with the constraints of coloring problems.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You're an expert problem solver specializing in combinatorics and counting problems.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "\"Given the constraints that no two adjacent dots may be the same color, solve the following math problem step-by-step without overcounting: {message}\"\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "The instruction is to modify the values of the variables in order to correct the logical process given by the feedback on the outputs. The code snippet involves formatting a user prompt and calling an LLM for a math-related task, specifically coloring a structure following certain rules. According to the feedback, the logic used by the LLM is incorrect in solving the given problem, which means that the provided instructions or prompts might not have guided the LLM appropriately.\n",
      "\n",
      "From the feedback, it is suggested that the calculation should involve breaking down a figure coloring task into parts, resulting in 54 ways, instead of the student's 216. The key misunderstanding is related to how coloring constraints apply and could indicate that the LLM wasn't prompted to consider the constraints effectively.\n",
      "\n",
      "Looking at `str36` and `str37`, these represent system and user prompts respectively. `str36` describes the general role of the system, and `str37` structures the specific problem statement addressed to the LLM. Enhancing these prompts' focus on key constraints and logic breakdown may lead to more accurate responses.\n",
      "\n",
      "To achieve this improvement, the focus of the LLM prompt should emphasize the specific constraints more clearly. `str36` should encourage logical and constraint-based reasoning for problem-solving. `str37` should explicitly remind about considering adjacency and coloring uniqueness to ensure those constraints are applied properly.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You're a logical mathematician specializing in solving step-by-step problems with attention to constraints and unique solutions.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem step-by-step, ensuring to consider adjacency and unique coloring constraints: {message}\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "\u001B[96mCandidate 1: Validation score: 0.0000\u001B[0m\n",
      "\u001B[92mKeeping all 1 candidates as num_candidates <= beam_width. Scores: ['0.0000']\u001B[0m\n",
      "\u001B[92mDepth 1 - Best validation score: 0.0000\u001B[0m\n",
      "\u001B[94m\n",
      "===== Beam Search Depth 2/2 with 1 beams =====\u001B[0m\n",
      "\u001B[96mSampled validation minibatch of size 1 for depth 2\u001B[0m\n",
      "\u001B[93mProcessing beam 1/1\u001B[0m\n",
      "LLM response:\n",
      " ```xml\n",
      "<reasoning>\n",
      "The instruction asks us to adjust the values of the variables to improve the output based on the feedback provided. The feedback indicates that the final answer of \\( \\boxed{864} \\) given by the student is incorrect as the correct answer is \\( \\boxed{54} \\). The error is in the calculation related to the constraints imposed by the edges connecting the triangles in the graph problem. The student incorrectly multiplied by an additional factor instead of considering these edges like additional triangles. \n",
      "\n",
      "The essential logic that needs to be corrected is articulated in the feedback: calculate the coloring possibilities by considering the constraints of connecting triangles directly, using \\( 6 \\cdot 3 \\cdot 3 = 54 \\).\n",
      "\n",
      "The system prompt (`str36`) provides context for making calculations, so it seems appropriate as is. However, the user prompt (`str37`) instructs the LLM to solve the problem without reminding it about the common pitfalls or the expected solution approach. As such, we can amend `str37` to direct the system on how to evaluate the constraints correctly, highlighting the need to consider the interacting constraints directly, as given in the feedback.\n",
      "\n",
      "Therefore, the suggested change is to adjust `str37` to guide the LLM to provide the correct calculation method.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem step-by-step, considering the connecting edges as additional triangles for constraint calculations: {message}\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "1. The #Instruction asks us to adjust the #Variables to improve the output based on the #Feedback provided.\n",
      "\n",
      "2. The #Feedback indicates that the current output, which is derived from the prompts contained in #Variables, leads the model to produce an incorrect answer. The key issue identified is in the student's calculation for a math problem related to graph coloring, specifically incorporating connecting edge constraints incorrectly.\n",
      "\n",
      "3. The #Feedback suggests a step-by-step correction for solving similar types of problems, emphasizing the need to account for triangle configurations and their constraints in graph coloring. \n",
      "\n",
      "4. To improve the output, the #Variables (specifically `str37`) can be adjusted to guide the model more effectively in handling such problems. It seems `str36` is a general system description and is appropriate for the context of answering math problems, so substantial changes might not be necessary here.\n",
      "\n",
      "5. However, `str37` should be modified to include more specific instructions or hints about ensuring the model correctly incorporates such constraints in its calculations.\n",
      "\n",
      "6. Based on the #Feedback, I'll adjust `str37` to include instructions about accounting for adjacent vertex constraints when coloring, which may aid in attaining the correct solution.\n",
      "</reasoning>\n",
      "\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem step-by-step. Ensure you account for constraints such as no two adjacent vertices can share the same color: {message}\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```xml\n",
      "<reasoning>\n",
      "The instruction requires adjusting the values of `str36` and `str37` to improve the code's output based on the feedback provided. The feedback indicates that the calculation performed by the model does not account for certain constraints correctly. The model's decision path should be guided by prompts that make it consider the correct method for evaluating connecting graphs as detailed in the feedback. `str37` appears to be a prompt template instructing the model on how to frame its response, whereas `str36` initializes the context for the model. Since the feedback specifies the precise step-by-step method to solve the problem accurately, reflecting these steps in the model's prompt might help. \n",
      "\n",
      "We need to incorporate specific details about coloring and constraints into the prompt message, encouraging the LLM to account for these aspects in its response. Suggested changes in `str37` include a direct emphasis on incorporating edge constraints when discussing graph coloring, while `str36` should frame a problem-solving context that specifically mentions critical thinking with respect to the constraints and interrelationships of graph compartments.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You're a helpful agent specializing in validating and solving complex math problems with emphasis on constraints and dependencies within graph structures.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Analyze and solve the following graph coloring problem step-by-step, considering constraints particularly the edge and vertex dependencies: {message}\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```\n",
      "<reasoning>\n",
      "The #Instruction asks us to adjust the values in #Variables to improve the output based on the feedback. The feedback indicates that the LLM's calculation of the math problem is incorrect and provides specific instructions on how to compute the correct answer. The incorrect output shows that the interaction constraints between the triangular coloring were not correctly calculated, leading to an incorrect total.\n",
      "\n",
      "In #Code, `str37` contains a template for the user prompt in which `message` should be replaced with `message47`, conveying the math problem that needs solving. `str36` is being used as the system prompt. To address the feedback and adjust the approach, we should ensure that the LLM specifically acknowledges and focuses on the need to properly consider constraints on the connecting edges, as pointed out in the feedback, in its problem-solving process.\n",
      "\n",
      "The erroneous computation suggests that even though the instruction might have been clear, additional emphasis on constraints handling might help guide the model to consider that aspect explicitly.\n",
      "\n",
      "Therefore, modify `str36` to instruct the system to emphasize addressing and checking for understanding of complex constraints. Also, clarify `str37` to encompass a more directed type of problem-solving focusing on constraints that arise due to the situation described.\n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You're a helpful agent answering math problems. Ensure thorough consideration of all constraints that arise in complex, multi-part problems, and focus on symmetrical patterns and interactions between connected components.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Solve the following math problem step-by-step, focusing especially on any constraints between interacting parts: {message}\n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "LLM response:\n",
      " ```xml\n",
      "<reasoning>\n",
      "The #Feedback indicates that the current approach to solving the problem does not appropriately consider the constraints imposed by the connecting edges between the triangles. The problem is essentially a combinatorial one involving graph coloring, and the essence of the error lies in calculating the \"ways\" to color the interconnected triangles correctly. The #Variables `str36` and `str37` represent the system and user prompts for the LLM, which generate the response. The issue likely stems from the insufficient detail or incorrect focus of the prompts, which fails to guide the model toward considering the interacting constraints.\n",
      "\n",
      "To correct the error, these prompts should be revised to better emphasize the graph's connecting constraints and their effect on the coloring possibilities. This requires multi-step logic that breaks down the problem using the specific case where connecting triangles or edges impose additional constraints to fulfill the requirement that no two adjacent points (or vertices) share the same color. The variables should guide the LLM to reason about symmetry and constraint interactions more explicitly, as explained in the feedback.\n",
      "\n",
      "The updated prompts in `str36` and `str37` would better set the context for the LLM, guiding it to output the correct logic for calculating the number of valid colorings. \n",
      "</reasoning>\n",
      "<variable>\n",
      "<name>str36</name>\n",
      "<value>\n",
      "You are a logic expert in graph theory and combinatorics, providing detailed solutions for complex graph coloring problems with constraints.\n",
      "</value>\n",
      "</variable>\n",
      "<variable>\n",
      "<name>str37</name>\n",
      "<value>\n",
      "Consider a graph with interconnected triangles. Find the number of ways to color the graph such that no two adjacent vertices share the same color. Break down the problem step-by-step: Color the first triangle, then apply constraints for each subsequent triangle in relation to the edges connecting them. \n",
      "</value>\n",
      "</variable>\n",
      "```\n",
      "\u001B[96mCandidate 1: Validation score: 0.0000\u001B[0m\n",
      "\u001B[92mKeeping all 1 candidates as num_candidates <= beam_width. Scores: ['0.0000']\u001B[0m\n",
      "\u001B[92mDepth 2 - Best validation score: 0.0000\u001B[0m\n",
      "\u001B[94m\n",
      "===== Final Selection Using Full Validation Set =====\u001B[0m\n",
      "\u001B[96mCandidate 1: Validation score: 0.0000\u001B[0m\n",
      "\u001B[92mKeeping all 1 candidates as num_candidates <= beam_width. Scores: ['0.0000']\u001B[0m\n",
      "\u001B[95m\n",
      "===== Final Proposal Candidate Parameters =====\u001B[0m\n",
      "\u001B[94mstr:36: You're a helpful agent answering math problems.\u001B[0m\n",
      "\u001B[94mstr:37: Solve the following math problem step-by-step: {message}\u001B[0m\n",
      "\u001B[92mBEST BEAM - Test score: 0.0000\u001B[0m\n",
      "\u001B[94m\n",
      "===== Periodic Test Scores Summary =====\u001B[0m\n",
      "\u001B[96mDepth 1: Test score = 0.0000\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "metrics, final_score = algorithm.train(**train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf70580-dfd8-4f07-9e9d-30ad0eae86d6",
   "metadata": {},
   "source": [
    "### UCB Search Demo\n",
    "\n",
    "![img](https://blogs.mathworks.com/images/loren/2016/multiarmedbandit.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb10d4d2-1d2a-4e40-ac2c-acad618bcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "math_data = datasets.load_dataset('xuanfeiren/math_hard_gemini')\n",
    "# math_data = datasets.load_dataset('gsm8k', 'main')\n",
    "\n",
    "num_train_samples = 2\n",
    "num_validate_samples = 5\n",
    "num_test_samples = 1\n",
    "\n",
    "# Select data subsets\n",
    "train_data = math_data['train'].select(\n",
    "    range(num_train_samples, num_train_samples + num_validate_samples)\n",
    ")\n",
    "validate_data = train_data\n",
    "test_data = math_data['test'].select(range(num_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d77fc511-8bf2-43d2-931c-f49023c6c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = \"gpt-4o\"\n",
    "\n",
    "student_llm = LLM()\n",
    "agent = Learner(llm=student_llm)\n",
    "\n",
    "train_guide = TeacherGuide(model=teacher_model)\n",
    "validate_guide = TeacherGuide(model=teacher_model)\n",
    "\n",
    "optimizer = OptoPrimeV2(agent.parameters())\n",
    "\n",
    "logger = SimpleLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b5a253e2-a7d3-470b-8942-7ab01b33edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_threads = 1\n",
    "batch_size = 2\n",
    "eval_frequency = 1\n",
    "log_frequency = 1\n",
    "validation_dataset_size = 5\n",
    "\n",
    "# Select data subsets\n",
    "train_data = math_data['train'].select(\n",
    "    range(num_train_samples, num_train_samples + num_validate_samples)\n",
    ")\n",
    "validate_data = train_data\n",
    "test_data = math_data['test'].select(range(num_test_samples))\n",
    "\n",
    "train_dataset = {'inputs': train_data['problem'], 'infos': train_data['solution']}\n",
    "validate_dataset = {'inputs': validate_data['problem'], 'infos': validate_data['solution']}\n",
    "test_dataset = {'inputs': test_data['problem'], 'infos': test_data['solution']}\n",
    "\n",
    "# train_dataset = {'inputs': train_data['question'], 'infos': train_data['answer']}\n",
    "# validate_dataset = {'inputs': validate_data['question'], 'infos': validate_data['answer']}\n",
    "# test_dataset = {'inputs': test_data['question'], 'infos': test_data['answer']}\n",
    "\n",
    "train_params = {\n",
    "    \"guide\": train_guide,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"num_threads\": 1,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"test_dataset\": test_dataset,\n",
    "    \"validate_dataset\": validate_dataset,\n",
    "    \"validate_guide\": validate_guide,\n",
    "    \"eval_frequency\": eval_frequency,\n",
    "    \"log_frequency\": log_frequency,\n",
    "    \"validation_dataset_size\": validation_dataset_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b3cb76b-8903-4853-8b34-00d46feca9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_exploration_factor = 1.0\n",
    "num_threads = 1\n",
    "max_buffer_size = 10\n",
    "\n",
    "algorithm = UCBSearchAlgorithm(\n",
    "    agent=agent,\n",
    "    optimizer=optimizer,\n",
    "    logger=logger,\n",
    "    num_threads=num_threads,\n",
    "    max_buffer_size=max_buffer_size,\n",
    "    ucb_exploration_factor=ucb_exploration_factor\n",
    ")\n",
    "\n",
    "num_search_iterations = 10\n",
    "train_batch_size_ucb = 1\n",
    "\n",
    "train_params.update({\n",
    "    \"num_search_iterations\": num_search_iterations,\n",
    "    \"train_batch_size\": train_batch_size_ucb,\n",
    "    \"evaluation_batch_size\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da0871b4-42b2-4e1d-aa32-1878da111654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[96mEvaluating initial parameters using validation_dataset samples...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at /Users/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.20.0/src/types/dict.rs:338:13:\n",
      "dictionary changed size during iteration\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mInitial candidate: Score 0.0000, Evals 2\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-inf, inf) [mean: 0.0000, n: 0]\u001B[0m\n",
      "\u001B[94mIter 1/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 1: Best score in buffer: 0.0000, Buffer size: 2, Total samples: 2\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-0.8326, 0.8326) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 2: (-0.8326, 0.8326) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 2/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 2: Best score in buffer: 0.0000, Buffer size: 3, Total samples: 4\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-0.8326, 0.8326) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.1774, 1.1774) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.1774, 1.1774) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 3/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 3: Best score in buffer: 0.0000, Buffer size: 4, Total samples: 6\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-0.9465, 0.9465) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-0.9465, 0.9465) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.3386, 1.3386) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.3386, 1.3386) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 4/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 4: Best score in buffer: 0.0000, Buffer size: 5, Total samples: 8\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-1.0197, 1.0197) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.0197, 1.0197) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.0197, 1.0197) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.4420, 1.4420) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 5: (-1.4420, 1.4420) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 5/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 5: Best score in buffer: 0.0000, Buffer size: 6, Total samples: 10\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-1.0730, 1.0730) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.0730, 1.0730) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.0730, 1.0730) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.0730, 1.0730) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 5: (-1.5174, 1.5174) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 6: (-1.5174, 1.5174) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 6/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 6: Best score in buffer: 0.0000, Buffer size: 7, Total samples: 12\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-1.1147, 1.1147) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.1147, 1.1147) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.1147, 1.1147) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.1147, 1.1147) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 5: (-1.1147, 1.1147) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 6: (-1.5764, 1.5764) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 7: (-1.5764, 1.5764) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 7/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 7: Best score in buffer: 0.0000, Buffer size: 8, Total samples: 14\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-1.1487, 1.1487) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.1487, 1.1487) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.1487, 1.1487) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.1487, 1.1487) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 5: (-1.1487, 1.1487) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 6: (-1.1487, 1.1487) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 7: (-1.6245, 1.6245) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 8: (-1.6245, 1.6245) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 8/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 8: Best score in buffer: 0.0000, Buffer size: 9, Total samples: 16\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 5: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 6: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 7: (-1.1774, 1.1774) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 8: (-1.6651, 1.6651) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 9: (-1.6651, 1.6651) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 9/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 9: Best score in buffer: 0.0000, Buffer size: 10, Total samples: 18\u001B[0m\n",
      "\u001B[96mConfidence intervals for all candidates:\u001B[0m\n",
      "\u001B[96mAction 1: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 2: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 3: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 4: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 5: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 6: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 7: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 8: (-1.2022, 1.2022) [mean: 0.0000, n: 2]\u001B[0m\n",
      "\u001B[96mAction 9: (-1.7001, 1.7001) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[96mAction 10: (-1.7001, 1.7001) [mean: 0.0000, n: 1]\u001B[0m\n",
      "\u001B[94mIter 10/10: \u001B[0m\n",
      "\u001B[92mLog @ Iter 10: Best score in buffer: 0.0000, Buffer size: 10, Total samples: 20\u001B[0m\n",
      "\u001B[94mUCB search finished.\u001B[0m\n",
      "\u001B[92mFinal best candidate: Mean Score 0.0000, Evals 2\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "metrics, final_score = algorithm.train(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be8bf5df-151a-4278-9f62-0234ae512ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c1f78-46f6-4140-8f3a-cc0372672427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0cb6e-394b-4139-b7a7-29c852e35816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1972d-bc8e-4fdb-a9e3-df58ee7cd764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c395e-971d-4bf8-a725-9e41537338eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8669b70-a4f3-4f0f-80f6-6ba1646dbefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e177f-cf3b-4d19-b932-2ed4b128436f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
