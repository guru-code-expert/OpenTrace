name: Full CI (with Ollama)

on:
  push:
    branches: [ main ]
  # Manual trigger for testing or major releases
  workflow_dispatch:

jobs:
  full-test:
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
  
    # 1) Restore any cached Ollama data
    - name: Restore Ollama cache
      uses: actions/cache@v4
      with:
        path: ~/.ollama
        key: qwen3-vl-2b-v1

    # 2) Install Ollama
    - name: Install Ollama
      run: |
        curl -fsSL https://ollama.com/install.sh | sh

    # 3) Drop-in override to bump context window to 4k tokens
    - name: Configure Ollama for 4K context
      run: |
        sudo mkdir -p /etc/systemd/system/ollama.service.d
        sudo tee /etc/systemd/system/ollama.service.d/override.conf << 'EOF'
        [Service]
        ExecStart=
        ExecStart=/usr/local/bin/ollama serve --num_ctx 4000
        EOF
        sudo systemctl daemon-reload

    # 4) Enable & start the systemd-managed Ollama daemon
    - name: Enable & start Ollama
      run: |
        sudo systemctl enable --now ollama

    # 5) Pull the qwen3-vl:2b model - supports multimodal/vision with OpenAI-compatible API
    - name: Pull qwen3-vl:2b model (multimodal)
      run: ollama pull qwen3-vl:2b

    # 6) Set up Python & install dependencies
    - uses: actions/setup-python@v5
      with: { python-version: "3.13" }
    - name: Install Python deps
      run: |
        pip install -e .
        pip install pytest datasets numpy

    # 7) Point LiteLLM/OpenAI to our local Ollama server
    - name: Configure LLM env
      run: |
        echo "OPENAI_API_KEY=ollama"             >> $GITHUB_ENV
        echo "OPENAI_API_BASE=http://localhost:11434/v1" >> $GITHUB_ENV
        echo "TRACE_LITELLM_MODEL=openai/qwen3-vl:2b" >> $GITHUB_ENV

    # 8) Run all Trace unit tests
    - name: Run unit tests
      run: pytest tests/unit_tests/

    # 9) Run basic tests for each optimizer (some will fail due to the small LLM model chosen for free GitHub CI)
#    - name: Run optimizers test suite
#      run: pytest tests/llm_optimizers_tests/test_optimizer.py || true
#      continue-on-error: true
